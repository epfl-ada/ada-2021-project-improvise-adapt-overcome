{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to run this notebook, one has to install the gensim and nltk packages:\n",
    "```\n",
    "conda install -c anaconda nltk\n",
    "conda install -c anaconda gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing topics from a cluster using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful libraries\n",
    "We will be using the Gensim and Natural Language Toolkit (NLTK) to help us process the quotes' text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "from gensim.models.phrases import Phrases\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = EnglishStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "We first preprocess the quotes:\n",
    "- Split text into words, apply lowercase, and remove punctuation\n",
    "- Ignore words of length < 3\n",
    "- Remove stopwords\n",
    "- Lemmatize words \n",
    "- Add bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_pos(tag):\n",
    "    'Convert a POS tag to its equivalent wordnet tag'\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return wn.NOUN # default option if all else fails\n",
    "\n",
    "def lemmatize(tagged):\n",
    "    'Apply lemmatization to a word'\n",
    "    # Decompose word and its tag\n",
    "    word, tag = tagged   \n",
    "    return lemmatizer.lemmatize(word, pos=wordnet_pos(tag))\n",
    "\n",
    "def stem(word):\n",
    "    'Apply stemming to a word'\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "def preprocess_quotes(qs):\n",
    "    'Split quote into words, apply lowercase, remove punctuation, ignore words of length < 3, remove stopwords'\n",
    "    processed_quotes = []\n",
    "    for q in qs:\n",
    "        processed = []\n",
    "        # Convert quote to list of lowercase tokens, ignoring those w/ length < 3\n",
    "        # Apply POS tagging\n",
    "        for token in pos_tag(simple_preprocess(q, min_len=3)):\n",
    "            # Ignore stopwords\n",
    "            if token[0] not in STOPWORDS:\n",
    "                # Lemmatize token\n",
    "                processed.append(lemmatize(token))\n",
    "        processed_quotes.append(processed)\n",
    "    \n",
    "    # Add bigrams\n",
    "    bigram = Phrases(processed_quotes, min_count=10, delimiter='_', threshold=5)\n",
    "    for i in range(len(processed_quotes)):\n",
    "        for token in bigram[processed_quotes[i]]:\n",
    "            if '_' in token:\n",
    "                # Add token to quote if it is a bigram\n",
    "                processed_quotes[i].append(token)\n",
    "    \n",
    "    return processed_quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: We can consider adding n-grams (bigrams at least) to bunch words that appear frequently together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce bag of words\n",
    "We then map our preprocessed quotes to produce a \"bag of words corpus\" which is a dictionary of words and their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_bow_corpus(processed_qs):\n",
    "    'Produce a bag of words corpus given processed quotes'\n",
    "    dictionary = corpora.Dictionary(processed_qs)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "    return [dictionary.doc2bow(q, allow_update=True) for q in processed_qs], dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading BOW corpus\n",
    "We provide code to save and load a corpus we produced to avoid repeating computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bow_corpus(bow_corpus, file_id='0'):\n",
    "    'Save a bag of words corpus with a given identifier'\n",
    "    corpora.MmCorpus.serialize('./data/BOW_corpus_{}.mm'.format(file_id), bow_corpus)\n",
    "    \n",
    "def load_bow_corpus(file_id='0'):\n",
    "    'Load a bag of words corpus with a given identifier'\n",
    "    return corpora.MmCorpus('./data/BOW_corpus_{}.mm'.format(file_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LDA to extract topics\n",
    "We now run LDA on our bag of words corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model(bow_corpus, id2word, n_topics):\n",
    "    return LdaMulticore(corpus=bow_corpus, num_topics=n_topics, id2word=id2word, workers=6, passes=10, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Pipeline\n",
    "def get_lda_model(quotes, n_topics):\n",
    "    '''Return LDA model'''\n",
    "    processed_quotes = preprocess_quotes(quotes)\n",
    "    bow, wordmap = produce_bow_corpus(processed_quotes)\n",
    "    model = lda_model(bow, wordmap, n_topics)\n",
    "    return model\n",
    "        \n",
    "def model_topics(model):\n",
    "    '''Show top 10 words for each topic'''\n",
    "    for topic_id, words in model.show_topics(formatted=False):\n",
    "        print(topic_id, [x[0] for x in words])\n",
    "        \n",
    "# --- SAVING AND LOADING LDA MODEL --- #\n",
    "def save_model(model, file_name):\n",
    "    model.save('../datasets/lda/'+file_name)\n",
    "    \n",
    "def load_model(file_name):\n",
    "    return LdaModel.load('../datasets/lda/'+file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the gensim documentation on LDA as reference: https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_QUOTES = '../data/clean_quotes.csv.bz2'\n",
    "CLUSTERS = '../data/clusters.csv.bz2'\n",
    "QUOTES_PATH = '../data/quotes-2020.json.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-24-000168</td>\n",
       "      <td>people.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-21-031706</td>\n",
       "      <td>people.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID     journal\n",
       "0  2020-01-24-000168  people.com\n",
       "3  2020-01-21-031706  people.com"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_quotes = pd.read_csv(CLEAN_QUOTES).drop_duplicates()[['quoteID', 'journal']]\n",
    "clean_quotes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "journal\n",
       "1011now.com      -1.0\n",
       "1070thefan.com   -1.0\n",
       "Name: cluster_id, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_assignments = pd.read_csv(CLUSTERS, index_col=0)['cluster_id']\n",
    "cluster_assignments.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered = cluster_assignments.groupby(cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 15\n"
     ]
    }
   ],
   "source": [
    "n_clusters = len(cluster_assignments.unique()) - 1 # ignore noise cluster (-1 assignemnt)\n",
    "print(f'Number of clusters: {n_clusters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [clustered.get_group(n) for n in range(n_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk\n",
      "Processing chunk\n",
      "Processing chunk\n",
      "Processing chunk\n",
      "Processing chunk\n",
      "Processing chunk\n",
      "Done processing!\n"
     ]
    }
   ],
   "source": [
    "quotes = []\n",
    "\n",
    "def process_chunk(chunk):\n",
    "        print(f'Processing chunk')\n",
    "        quotes.append(chunk[['quoteID', 'quotation']])      \n",
    "\n",
    "with pd.read_json(QUOTES_PATH, lines=True, compression='bz2', chunksize=1000000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        process_chunk(chunk)\n",
    "print('Done processing!')\n",
    "        \n",
    "quotes = pd.concat(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {Quotation ID -> Quotation} dictionary for fast retrieval\n",
    "quotes_dict = dict(quotes.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2020-04-06-037825\n",
       "1    2020-04-06-060329\n",
       "Name: quoteID, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Match quote IDs to their clusters\n",
    "ids = [clean_quotes.merge(groups[i], on='journal')['quoteID'] for i in range(len(groups))]\n",
    "ids[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Right now we're just reacting to... it's a different retail chain, whether we could get physical copies to people, is the internet infrastructure there to support all countries... We're right now looking at all sorts of different options,\",\n",
       " \"We'd rather put our focus on finishing the actual game and getting it to people,\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain cluster quotes\n",
    "cluster_quotes = [[quotes_dict[id_] for id_ in ids[i]] for i in range(len(ids))]\n",
    "cluster_quotes[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: 2602 quotes\n",
      "Cluster 1: 15305 quotes\n",
      "Cluster 2: 44992 quotes\n",
      "Cluster 3: 85009 quotes\n",
      "Cluster 4: 24309 quotes\n",
      "Cluster 5: 25059 quotes\n",
      "Cluster 6: 196466 quotes\n",
      "Cluster 7: 504899 quotes\n",
      "Cluster 8: 1309483 quotes\n",
      "Cluster 9: 9287 quotes\n",
      "Cluster 10: 36923 quotes\n",
      "Cluster 11: 335119 quotes\n",
      "Cluster 12: 239235 quotes\n",
      "Cluster 13: 12086 quotes\n",
      "Cluster 14: 2163368 quotes\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cluster_quotes)):\n",
    "    print(f'Cluster {i}: {len(cluster_quotes[i])} quotes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cluster 0\n",
      "Saving LDA model...\n",
      "Processing cluster 1\n",
      "Saving LDA model...\n",
      "Processing cluster 2\n",
      "Saving LDA model...\n",
      "Processing cluster 3\n",
      "Saving LDA model...\n",
      "Processing cluster 4\n",
      "Saving LDA model...\n",
      "Processing cluster 5\n",
      "Saving LDA model...\n",
      "Processing cluster 6\n",
      "Saving LDA model...\n",
      "Processing cluster 7\n",
      "Saving LDA model...\n",
      "Processing cluster 8\n",
      "Saving LDA model...\n",
      "Processing cluster 9\n",
      "Saving LDA model...\n",
      "Processing cluster 10\n",
      "Saving LDA model...\n",
      "Processing cluster 11\n",
      "Saving LDA model...\n",
      "Processing cluster 12\n",
      "Saving LDA model...\n",
      "Processing cluster 13\n",
      "Saving LDA model...\n",
      "Processing cluster 14\n",
      "Saving LDA model...\n"
     ]
    }
   ],
   "source": [
    "# Run LDA on each cluster\n",
    "for i in range(n_clusters):\n",
    "    print(f'Processing cluster {i}')\n",
    "    model = get_lda_model(cluster_quotes[i], 4)\n",
    "    print(f'Saving LDA model...')\n",
    "    save_model(model, f'cluster-{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 topics:\n",
      "0 ['like', 'thing', 'think', 'go', 'year', 'look', 'different', 'good', 'new', 'console']\n",
      "1 ['game', 'want', 'work', 'time', 'experience', 'new', 'people', 'like', 'go', 'year']\n",
      "2 ['game', 'people', 'new', 'play', 'year', 'lot', 'video', 'work', 'like', 'know']\n",
      "3 ['switch', 'nintendo', 'time', 'story', 'nintendo_switch', 'like', 'world', 'new', 'people', 'team']\n",
      "Cluster 1 topics:\n",
      "0 ['life', 'people', 'god', 'time', 'know', 'new', 'think', 'go', 'world', 'human']\n",
      "1 ['church', 'people', 'lord', 'god', 'good', 'catholic', 'pray', 'community', 'woman', 'day']\n",
      "2 ['god', 'life', 'love', 'time', 'human', 'way', 'need', 'jesus', 'church', 'christ']\n",
      "3 ['people', 'come', 'world', 'right', 'holy', 'want', 'god', 'country', 'today', 'heart']\n",
      "Cluster 2 topics:\n",
      "0 ['people', 'need', 'time', 'family', 'health', 'public', 'work', 'support', 'country', 'virus']\n",
      "1 ['want', 'come', 'year', 'play', 'level', 'great', 'club', 'time', 'game', 'continue']\n",
      "2 ['government', 'people', 'party', 'change', 'want', 'election', 'country', 'day', 'new', 'life']\n",
      "3 ['go', 'think', 'get', 'like', 'good', 'time', 'know', 'come', 'thing', 'game']\n",
      "Cluster 3 topics:\n",
      "0 ['player', 'team', 'club', 'work', 'best', 'play', 'world', 'good', 'want', 'great']\n",
      "1 ['play', 'go', 'game', 'think', 'league', 'good', 'win', 'know', 'player', 'team']\n",
      "2 ['think', 'time', 'thing', 'know', 'come', 'game', 'want', 'go', 'good', 'look']\n",
      "3 ['game', 'like', 'season', 'good', 'great', 'player', 'team', 'happy', 'play', 'get']\n",
      "Cluster 4 topics:\n",
      "0 ['think', 'team', 'player', 'want', 'win', 'season', 'right', 'thing', 'play', 'get']\n",
      "1 ['play', 'get', 'game', 'lot', 'know', 'great', 'go', 'think', 'people', 'time']\n",
      "2 ['go', 'good', 'year', 'think', 'play', 'guy', 'like', 'look', 'game', 'want']\n",
      "3 ['go', 'like', 'know', 'little', 'get', 'think', 'come', 'good', 'time', 'try']\n",
      "Cluster 5 topics:\n",
      "0 ['player', 'play', 'want', 'come', 'coach', 'lot', 'year', 'like', 'go', 'best']\n",
      "1 ['know', 'team', 'look', 'go', 'year', 'lot', 'guy', 'like', 'super', 'game']\n",
      "2 ['think', 'great', 'team', 'get', 'like', 'guy', 'time', 'thing', 'go', 'look']\n",
      "3 ['go', 'play', 'think', 'good', 'game', 'get', 'like', 'guy', 'know', 'year']\n",
      "Cluster 6 topics:\n",
      "0 ['like', 'go', 'think', 'know', 'thing', 'want', 'people', 'feel', 'get', 'say']\n",
      "1 ['movie', 'film', 'time', 'know', 'think', 'love', 'go', 'get', 'happy', 'like']\n",
      "2 ['love', 'life', 'family', 'people', 'woman', 'world', 'time', 'know', 'thank', 'work']\n",
      "3 ['song', 'new', 'music', 'work', 'year', 'play', 'write', 'time', 'come', 'character']\n",
      "Cluster 7 topics:\n",
      "0 ['go', 'think', 'get', 'good', 'time', 'like', 'know', 'play', 'come', 'year']\n",
      "1 ['people', 'need', 'want', 'home', 'stay', 'community', 'school', 'child', 'service', 'government']\n",
      "2 ['state', 'government', 'people', 'need', 'work', 'minister', 'business', 'test', 'come', 'australia']\n",
      "3 ['people', 'health', 'need', 'time', 'work', 'community', 'australian', 'continue', 'government', 'virus']\n",
      "Cluster 8 topics:\n",
      "0 ['think', 'go', 'play', 'good', 'game', 'get', 'know', 'player', 'time', 'come']\n",
      "1 ['people', 'country', 'party', 'family', 'time', 'right', 'government', 'want', 'year', 'friend']\n",
      "2 ['people', 'spread', 'virus', 'need', 'go', 'time', 'long', 'week', 'place', 'new']\n",
      "3 ['people', 'work', 'need', 'government', 'support', 'help', 'health', 'country', 'home', 'time']\n",
      "Cluster 9 topics:\n",
      "0 ['work', 'people', 'way', 'country', 'know', 'need', 'change', 'new', 'virus', 'study']\n",
      "1 ['people', 'time', 'need', 'covid', 'year', 'health', 'world', 'case', 'test', 'government']\n",
      "2 ['like', 'time', 'go', 'need', 'africa', 'want', 'technology', 'future', 'new', 'think']\n",
      "3 ['year', 'like', 'look', 'time', 'know', 'people', 'thing', 'work', 'patient', 'need']\n",
      "Cluster 10 topics:\n",
      "0 ['think', 'go', 'play', 'lot', 'game', 'good', 'get', 'thing', 'people', 'time']\n",
      "1 ['people', 'know', 'new', 'like', 'need', 'housing', 'thing', 'life', 'home', 'go']\n",
      "2 ['want', 'like', 'know', 'time', 'work', 'people', 'year', 'look', 'go', 'need']\n",
      "3 ['health', 'work', 'time', 'public', 'go', 'people', 'come', 'need', 'help', 'like']\n",
      "Cluster 11 topics:\n",
      "0 ['people', 'government', 'india', 'need', 'state', 'country', 'covid', 'coronavirus', 'health', 'work']\n",
      "1 ['people', 'government', 'delhi', 'bjp', 'party', 'state', 'country', 'minister', 'say', 'issue']\n",
      "2 ['india', 'life', 'woman', 'country', 'world', 'love', 'year', 'new', 'work', 'indian']\n",
      "3 ['think', 'play', 'time', 'go', 'good', 'like', 'team', 'know', 'get', 'come']\n",
      "Cluster 12 topics:\n",
      "0 ['need', 'government', 'health', 'people', 'canadian', 'country', 'public', 'canada', 'work', 'community']\n",
      "1 ['work', 'need', 'forward', 'look', 'people', 'new', 'think', 'time', 'right', 'decision']\n",
      "2 ['go', 'think', 'know', 'like', 'get', 'play', 'lot', 'good', 'thing', 'game']\n",
      "3 ['people', 'want', 'life', 'family', 'go', 'home', 'tell', 'time', 'know', 'love']\n",
      "Cluster 13 topics:\n",
      "0 ['need', 'new', 'go', 'state', 'york', 'new_york', 'city', 'work', 'community', 'lot']\n",
      "1 ['new', 'people', 'know', 'test', 'work', 'york', 'go', 'new_york', 'care', 'life']\n",
      "2 ['go', 'want', 'get', 'think', 'people', 'time', 'like', 'year', 'good', 'thing']\n",
      "3 ['new', 'people', 'need', 'time', 'continue', 'state', 'work', 'help', 'health', 'day']\n",
      "Cluster 14 topics:\n",
      "0 ['president', 'trump', 'american', 'state', 'people', 'vote', 'house', 'election', 'party', 'want']\n",
      "1 ['people', 'need', 'care', 'life', 'family', 'home', 'help', 'work', 'pay', 'business']\n",
      "2 ['new', 'state', 'need', 'health', 'virus', 'test', 'work', 'public', 'time', 'country']\n",
      "3 ['go', 'think', 'know', 'like', 'thing', 'get', 'people', 'good', 'want', 'time']\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_clusters):\n",
    "    print(f'Cluster {i} topics:')\n",
    "    model_topics(load_model(f'cluster-{i}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordclouds\n",
    "We generate wordclouds for our cluster topics to visually represent our LDA output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate wordclouds for each cluster\n",
    "for i in range(n_clusters):\n",
    "    lda_model = load_model(f'cluster-{i}')\n",
    "    for t in range(lda_model.num_topics):\n",
    "        plt.figure()\n",
    "        plt.imshow(WordCloud(background_color='white').fit_words(dict(lda_model.show_topic(t, 200))))\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f'../datasets/wordclouds/cluster{i}_wordcloud{t}')\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
