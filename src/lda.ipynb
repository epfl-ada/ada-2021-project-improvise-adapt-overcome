{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to run this notebook, one has to install the gensim and nltk packages:\n",
    "```\n",
    "conda install -c anaconda nltk\n",
    "conda install -c anaconda gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing topics from a cluster using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We approximate a cluster that we might find by the sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('./data/quotes-2019-nytimes.json.bz2', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful data to us is the quotations' text only\n",
    "quotes = data['quotation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful libraries\n",
    "We will be using the Gensim and Natural Language Toolkit (NLTK) to help us process the quotes' text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Julian\n",
      "[nltk_data]     Blackwell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = EnglishStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "We first preprocess the quotes:\n",
    "- Split text into words, apply lowercase, and remove punctuation\n",
    "- Ignore words of length < 3\n",
    "- Remove stopwords\n",
    "- Lemmatize words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word):\n",
    "    'Apply lemmatization to a word'\n",
    "    # Important note: For the moment a default tag 'n' for nouns is used\n",
    "    # To improve: find and set the correct tag for each word\n",
    "    return lemmatizer.lemmatize(word, pos='n')\n",
    "\n",
    "def stem(word):\n",
    "    'Apply stemming to a word'\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "def preprocess_quotes(qs):\n",
    "    'Split quote into words, apply lowercase, remove punctuation, ignore words of length <= 3, remove stopwords'\n",
    "    processed_quotes = []\n",
    "    for q in qs:\n",
    "        processed = []\n",
    "        # Convert quote to list of lowercase tokens, ignoring those w/ length < 3\n",
    "        for token in simple_preprocess(q, min_len=3):\n",
    "            # Ignore stopwords\n",
    "            if token not in STOPWORDS:\n",
    "                # Lemmatize and stem token\n",
    "                processed.append(lemmatize(token))\n",
    "        processed_quotes.append(processed)\n",
    "                \n",
    "    return processed_quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: We can consider adding n-grams (bigrams at least) to bunch words that appear frequently together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce bag of words\n",
    "We then map our preprocessed quotes to produce a \"bag of words corpus\" which is a dictionary of words and their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_bow_corpus(processed_qs):\n",
    "    'Produce a bag of words corpus given processed quotes'\n",
    "    dictionary = corpora.Dictionary()\n",
    "    return [dictionary.doc2bow(q, allow_update=True) for q in processed_qs], dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading BOW corpus\n",
    "We provide code to save and load a corpus we produced to avoid repeating computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bow_corpus(bow_corpus, file_id='0'):\n",
    "    'Save a bag of words corpus with a given identifier'\n",
    "    corpora.MmCorpus.serialize('./data/BOW_corpus_{}.mm'.format(file_id), bow_corpus)\n",
    "    \n",
    "def load_bow_corpus(file_id='0'):\n",
    "    'Load a bag of words corpus with a given identifier'\n",
    "    return corpora.MmCorpus('./data/BOW_corpus_{}.mm'.format(file_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LDA to extract topics\n",
    "We now run LDA on our bag of words corpus.  \n",
    "**Note**: We will have to tune some parameters for the LDA model, mainly:\n",
    "- $\\alpha$: The a-priori belief on document-topic distribution\n",
    "- $\\eta$: The a-priori belief on topic-word distribution\n",
    "- `n_topics`: The number of topics to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model(bow_corpus, id2word, n_topics):\n",
    "    return LdaModel(corpus=bow_corpus, num_topics=n_topics, id2word=id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_quotes_topics(quotes, n_topics):\n",
    "    processed_quotes = preprocess_quotes(quotes)\n",
    "    bow, wordmap = produce_bow_corpus(processed_quotes)\n",
    "    model = lda_model(bow, wordmap, n_topics)\n",
    "    for topic_id, words in model.show_topics(formatted=False):\n",
    "        print(topic_id, [x[0] for x in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ['want', 'year', 'actually', 'election', 'seen', 'number', 'people', 'market', 'talking', 'chance']\n",
      "5 ['right', 'talk', 'trump', 'art', 'far', 'china', 'people', 'left', 'happened', 'human']\n",
      "4 ['got', 'wanted', 'coming', 'knew', 'percent', 'hand', 'help', 'open', 'kind', 'going']\n",
      "9 ['american', 'history', 'little', 'great', 'moment', 'opportunity', 'hope', 'bit', 'situation', 'best']\n",
      "2 ['person', 'going', 'come', 'win', 'wasn', 'question', 'like', 'mind', 'took', 'vote']\n",
      "8 ['state', 'president', 'house', 'love', 'united', 'fact', 'country', 'white', 'support', 'went']\n",
      "3 ['like', 'time', 'day', 'said', 'look', 'end', 'tell', 'year', 'money', 'felt']\n",
      "6 ['long', 'change', 'care', 'time', 'future', 'business', 'start', 'term', 'health', 'stop']\n",
      "7 ['new', 'trump', 'job', 'deal', 'high', 'york', 'night', 'level', 'plan', 'justice']\n",
      "1 ['woman', 'political', 'trying', 'problem', 'case', 'today', 'decision', 'law', 'better', 'campaign']\n"
     ]
    }
   ],
   "source": [
    "lda_quotes_topics(quotes, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are some topics that LDA extracted which we can use to try and label our cluster (in this case there seems to be a bias towards political/election news, along with some business and justice information). An observation here is that adding bigrams would have likely shown \"New York\" as a prevalent term if one has a look at topic 7 where \"new\" and \"york\" are present. Some optimization will have to be made in terms of how many topics we are trying produce to accurately describe a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the gensim documentation on LDA as reference: https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
