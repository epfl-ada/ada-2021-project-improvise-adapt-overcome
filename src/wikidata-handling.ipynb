{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4046ba26",
   "metadata": {
    "id": "Rh9HNGSyY-kr"
   },
   "source": [
    "# ADA Project Milestone 2 - Wikidata dataset newspaper generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85381fe",
   "metadata": {
    "id": "OmVyXMtcowtX"
   },
   "source": [
    "## Setup and Remote dataset loading\n",
    "\n",
    "We first import necessary libraries into the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5400b3c7",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1636737257809,
     "user": {
      "displayName": "Luca Bataillard",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09240833841167909174"
     },
     "user_tz": -60
    },
    "id": "VjWVrwYpc1TR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pywikibot as pw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236fd247",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7872,
     "status": "ok",
     "timestamp": 1636737266585,
     "user": {
      "displayName": "Luca Bataillard",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09240833841167909174"
     },
     "user_tz": -60
    },
    "id": "IeLtqc7vAXVp",
    "outputId": "ce2c7c15-7134-4e0f-e3ca-70b7374b9413"
   },
   "outputs": [],
   "source": [
    "%pip install pandas==1.3.0\n",
    "%pip install tld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa1442",
   "metadata": {
    "id": "jEZb44cqcz68"
   },
   "source": [
    "We mount the EPFL google drive and define access paths for the different datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8334245",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2657,
     "status": "ok",
     "timestamp": 1636737328496,
     "user": {
      "displayName": "Luca Bataillard",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09240833841167909174"
     },
     "user_tz": -60
    },
    "id": "b6825e50",
    "outputId": "4c6aaa5f-6794-4e38-a058-ee953e0d1f51"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf0bb10f",
   "metadata": {
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1636737352790,
     "user": {
      "displayName": "Luca Bataillard",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09240833841167909174"
     },
     "user_tz": -60
    },
    "id": "oO-QPsZ6Zbec"
   },
   "outputs": [],
   "source": [
    "BASE_PATH = \"../data/mnt/ada/\"\n",
    "#BASE_PATH = \"/content/drive/Shareddrives/Improvise ADApt Overcome/Datasets/\"\n",
    "\n",
    "SPEAKER_PATH = BASE_PATH + \"speakers/\"\n",
    "QUOTEBANK_PATH = BASE_PATH + \"quotebank/\"\n",
    "WIKI_PATH = BASE_PATH + \"wikipedia/\"\n",
    "NEWS_PATH = BASE_PATH + \"newspapers/\"\n",
    "\n",
    "SPEAKER_ATTRS = SPEAKER_PATH + \"speaker_attributes.parquet\"\n",
    "WIKIPEDIA_ATTRS = SPEAKER_PATH + \"wikidata_labels_descriptions.csv.bz2\"\n",
    "QB_WIKIPEDIA_ATTRS = SPEAKER_PATH + \"wikidata_labels_descriptions_quotebank.csv.bz2\"\n",
    "FULL_WIKIDUMP = WIKI_PATH + \"wikidata-20211004-all.json.gz\"\n",
    "WIKI_URLS = WIKI_PATH + \"wikiurls.json\"\n",
    "\n",
    "CLEAN_WIKI_URLS = WIKI_PATH + \"clean_urls.json\"\n",
    "CLEAN_QUOTES = QUOTEBANK_PATH + \"clean_quotes.csv.bz2\"\n",
    "WIKI_QUOTES = NEWS_PATH + \"clean_wiki_quotes.csv.bz2\"\n",
    "JOURNAL_WIKIDATA = WIKI_PATH + \"journal_wikidump.json\"\n",
    "JOURNAL_PROPS = WIKI_PATH + \"journal_props.json\"\n",
    "JOURNAL_ATTRS = NEWS_PATH + \"journal_attributes.json\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402acbff",
   "metadata": {
    "id": "HYktWM7tFjN8"
   },
   "source": [
    "To classify speakers, we will use the `speaker_attributes` parquet file as is without further modification. Its size is small enough to be managable in RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d48bbfc",
   "metadata": {
    "id": "VCc_QrfrF0Rn"
   },
   "source": [
    "## Newspaper URL extraction\n",
    "\n",
    "We need to generate a dataset linking news agency urls with their respective wikidata entry ids. This will allow us in the future to find patterns in the groups of news outlets by having more data about them. Once the wikidata id is obtained, it will relatively easy to obtain more information about those media outlets.\n",
    "\n",
    "Such a table is much reduced in size compared to a full wikipedia dump, since entries are restricted to news organizations. This means the dataset can be easily used in RAM, indexed by url in pandas, as a lookup table for publisher identifiers. This is a parallelizable task, so quotebank can be split into managable chunks to add a link between quote id and publisher for each quote. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757b09b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 16132,
     "status": "error",
     "timestamp": 1636737375151,
     "user": {
      "displayName": "Luca Bataillard",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09240833841167909174"
     },
     "user_tz": -60
    },
    "id": "3V2opxwcdlUm",
    "outputId": "2a8d8956-16e2-4ead-9f7d-67d54b6168ec"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse\n",
    "from tld import get_fld\n",
    "\n",
    "website_prop = \"P856\"\n",
    "\n",
    "def has_website(s):\n",
    "  return len(s.get(\"claims\", {}).get(website_prop, [])) > 0\n",
    "\n",
    "def extract_urls(s):\n",
    "  \"\"\"Takes a JSON wikidata entry and returns the list of official websites \n",
    "  linkedto that entry. The websises are returned as unique first-level-domains: \n",
    "  'https://test.google.com/exam/pl/e' becomes 'google.com'. Returns an empty \n",
    "  list if no urls\"\"\"\n",
    "\n",
    "    # Contains an official website?\n",
    "    if len(s.get(\"claims\", {}).get(website_prop, [])) > 0:\n",
    "        urls = []\n",
    "        flds = []\n",
    "\n",
    "    # For each website, add fld to urls array\n",
    "    for v in s[\"claims\"][website_prop]:\n",
    "        if (v[\"mainsnak\"].get(\"datavalue\", {}).get(\"value\", None) is not None):\n",
    "            url = urlparse(v[\"mainsnak\"][\"datavalue\"][\"value\"]).netloc\n",
    "\n",
    "            fld = get_fld(v[\"mainsnak\"][\"datavalue\"][\"value\"], fail_silently=True)\n",
    "\n",
    "            if fld and fld not in flds:\n",
    "                flds.append(fld)\n",
    "\n",
    "            if url not in urls:\n",
    "                urls.append(url)\n",
    "\n",
    "            return urls, flds\n",
    "        else:\n",
    "            return [], []\n",
    "\n",
    "\n",
    "def extract_newspaper_urls(inputf, outputf):\n",
    "    \"\"\"Takes an input wikidata dump file and writes a list of newspaper website\n",
    "    URLs. It filters entries based on if they are media companies. \n",
    "    The output file is a list of json objects, where each line is a json object. \n",
    "    Each object in the output file references a news agency, with \n",
    "    - \"id\" wikidata identifier of the news agency\n",
    "    - \"label\" wikidata label of the news agency\n",
    "    - \"websites\" list of urls related to that news agency\"\"\"\n",
    "    # Do not enforce encoding here since the input encoding is correct\n",
    "    with open(outputf, \"w\") as output_file:\n",
    "        with gzip.open(inputf, 'rb') as s_file:\n",
    "            for instance in s_file:\n",
    "                instance = instance.decode('utf-8')\n",
    "                instance = instance[:-2]\n",
    "                if len(instance)==0:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    s = json.loads(instance.strip(\"\\n\"))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                if s.get(\"labels\", {}).get(\"en\") is not None:\n",
    "                    s[\"label\"] = s[\"labels\"][\"en\"][\"value\"]\n",
    "\n",
    "                if s.get(\"labels\") is not None:\n",
    "                    del s[\"labels\"]\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                # Only take wiki entries with a website into consideration\n",
    "                if not has_website(s):\n",
    "                    continue\n",
    "\n",
    "                # Extract Official website \n",
    "                s[\"websites\"], s[\"flds\"] = extract_urls(s)\n",
    "\n",
    "                # Remove leftovers and unnecessary attributes\n",
    "                if s.get(\"aliases\") is not None:\n",
    "                    del s[\"aliases\"]\n",
    "                if s.get(\"descriptions\") is not None:\n",
    "                    del s[\"descriptions\"]\n",
    "                if s.get(\"sitelinks\") is not None:\n",
    "                    del s[\"sitelinks\"]\n",
    "                if s.get(\"claims\") is not None:\n",
    "                    del s[\"claims\"]\n",
    "                if s.get(\"lastrevid\") is not None:\n",
    "                    del s[\"lastrevid\"]\n",
    "                if s.get(\"type\") is not None:\n",
    "                    del s[\"type\"]\n",
    "\n",
    "                output_file.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "extract_newspaper_urls(FULL_WIKIDUMP, WIKI_URLS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50baf018",
   "metadata": {},
   "source": [
    "Go through every entry in the Wiki URL data to flatten `{id: ..., label: ..., flds: [...]}` into a list of `{id: ..., label: ..., flds: ...}, ...`. This is done after the fact due to the extremely long running time of the previous operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf64ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WIKI_URLS) as url_file, open(CLEAN_WIKI_URLS, 'w') as clean_urls:\n",
    "    for line in url_file:\n",
    "        urlDict = json.loads(line)\n",
    "        \n",
    "        for url in urlDict[\"flds\"]:\n",
    "            new_url = {\n",
    "                \"id\": urlDict[\"id\"],\n",
    "                \"url\": url\n",
    "            }\n",
    "            \n",
    "            if \"label\" in urlDict:\n",
    "                new_url[\"label\"] = urlDict[\"label\"]\n",
    "            \n",
    "            clean_urls.write(json.dumps(new_url, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fcaf14",
   "metadata": {},
   "source": [
    "We load the flattened URL mapping and index by URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd84852",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_urls = pd.read_json(CLEAN_WIKI_URLS, lines=True)\n",
    "wiki_urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d966084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_urls.set_index('url', inplace=True)\n",
    "wiki_urls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37937d8",
   "metadata": {},
   "source": [
    "## Merging quotes and URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7cc337",
   "metadata": {},
   "source": [
    "We now need to add a wikidata id columns to the quotes dataset. This is done by filtering the wiki urls\n",
    "to keep only the URLS that appear in the quote dataset. We then left outer join the quotes and the wiki urls on\n",
    "the URL column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d6202",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = pd.read_csv(CLEAN_QUOTES)\n",
    "quotes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b675e",
   "metadata": {},
   "source": [
    "Keep only the wikiurl entries appearing in the quotes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb667e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_urls = pd.DataFrame(quotes.journal.unique(), columns=['url'])\n",
    "\n",
    "journals = journal_urls.merge(wiki_urls, left_on='url', right_index=True, how='left')\n",
    "journals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814e4883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#journals.drop_duplicates(subset=['url'])\n",
    "#journals.head()\n",
    "journals.query('url == \"foxnews.com\"').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42727311",
   "metadata": {},
   "outputs": [],
   "source": [
    "journals.rename(columns={'url': 'journal', 'id': 'journal_id', 'label': 'journal_label'}, inplace=True)\n",
    "journals.set_index('journal', inplace=True)\n",
    "journals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2042e000",
   "metadata": {},
   "source": [
    "Now that we have a URL -> QID mapping, we will query wikipedia to extract hopefully meaningful features into the dataset. We do notice that the same URL can direct to several wikipedia entries. We will take care of this in a later part of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e96cc41-d8c0-4bd6-85c1-1a323b60ad15",
   "metadata": {},
   "source": [
    "## Enhance quote dataset with source journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcbe88-f8bb-4873-8ac6-0cc99d9ba3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_quotes = quotes.merge(journals, left_on='journal', how='left', right_index=True)\n",
    "wiki_quotes.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29db9a-086a-4a20-b5c6-d3502cddfb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_wiki = 1 - wiki_quotes.journal_id.isna().sum() / len(wiki_quotes.journal_id)\n",
    "print(f\"{pct_wiki * 100:.2f}% of all quotes have a wikipedia editor identified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272c895f-e92f-4ca6-a0b8-a47a6677f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_quotes.to_csv(WIKI_QUOTES, compression='bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce379562",
   "metadata": {},
   "source": [
    "## Generate journal enhanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab1cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_wiki_entry(qid):\n",
    "    if isinstance(qid, float):\n",
    "        return \"\", [] \n",
    "    \n",
    "    try:\n",
    "        site = pw.Site(\"wikidata\", \"wikidata\")\n",
    "        repo = site.data_repository()\n",
    "\n",
    "        item = pw.ItemPage(repo, qid).get()\n",
    "        desc = item.get(\"descriptions\", {}).get('en', \"\")\n",
    "        claims = item[\"claims\"].toJSON()\n",
    "\n",
    "        return desc, claims\n",
    "    except KeyboardInterrupt:\n",
    "        raise KeyboardInterrupt\n",
    "    except:\n",
    "        print(f\"Failed at {qid}\")\n",
    "        return \"\", []\n",
    "\n",
    "def get_item_claims_from_wiki(qid):    \n",
    "    _, claims = download_wiki_entry(qid)\n",
    "    \n",
    "    return claims "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1b8cf-1cc4-4858-90fa-251cc2c0bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_wikidump(file, journal, desc, claims):    \n",
    "    output = {}\n",
    "    \n",
    "    output[\"journal\"] = journal.journal\n",
    "    output[\"journal_ids\"] = journal.journal_id\n",
    "    output[\"journal_label\"] = journal.journal_label\n",
    "    output[\"description\"] = desc\n",
    "    output[\"claims\"] = claims\n",
    "    \n",
    "    file.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(JOURNAL_WIKIDATA, 'w') as fjournals:        \n",
    "    for _, journal in tqdm(journals.reset_index().iterrows(), total=len(journals)):\n",
    "        desc, claims = download_wiki_entry(journal.journal_id)\n",
    "        write_wikidump(fjournals, journal, desc, claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e8419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def year_extractor(raw_prop_val):    \n",
    "    if not (raw_prop_val and 'time' in raw_prop_val):\n",
    "        return np.nan\n",
    "    \n",
    "    try:\n",
    "        raw_year = raw_prop_val['time'].split('-')[0]\n",
    "        return int(raw_year.lstrip('+').lstrip('0'))\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0860b332",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Property:\n",
    "    def __init__(self, prop_id, label, is_item=True, extractor=None):\n",
    "        self.prop_id = prop_id\n",
    "        self.label = label\n",
    "        self.is_item = is_item\n",
    "        self.extractor = extractor\n",
    "        \n",
    "    def extract(self, raw_prop_value):\n",
    "        if self.extractor:\n",
    "            return self.extractor(raw_prop_value)\n",
    "        elif self.is_item:\n",
    "            return \"Q\" + str(raw_prop_value.get(\"numeric-id\"))\n",
    "        else:\n",
    "            return raw_prop_value\n",
    "            \n",
    "    def find_online(self, qid, repo=None):\n",
    "        claims = get_item_claims_from_wiki(qid, repo=repo)\n",
    "        return self.find_in_claims(claims)\n",
    "        \n",
    "    def find_in_claims(self, claims):\n",
    "        if len(claims) == 0:\n",
    "            return []\n",
    "    \n",
    "        prop = claims.get(self.prop_id, [])\n",
    "        values = []\n",
    "    \n",
    "        for val in prop:\n",
    "            value = val.get('mainsnak', {}) \\\n",
    "                       .get(\"datavalue\", {}) \\\n",
    "                       .get(\"value\", None)\n",
    "            \n",
    "            if value:\n",
    "                values.append(self.extract(value))\n",
    "        \n",
    "        return values\n",
    "\n",
    "\n",
    "properties = [\n",
    "    Property(\"P31\",\"instance of\"),\n",
    "    Property(\"P361\", \"part of\"),\n",
    "    Property(\"P136\", \"genre\"),\n",
    "    Property(\"P449\", \"original broadcaster\"),\n",
    "    Property(\"P17\", \"country\"),\n",
    "    Property(\"P495\", \"country of origin\"),\n",
    "    Property(\"P127\", \"owned by\"),\n",
    "    Property(\"P159\", \"headquaters location\"),\n",
    "    Property(\"P571\", \"inception_year\", extractor=year_extractor),\n",
    "    Property(\"P3912\", \"newspaper format\"),\n",
    "    Property(\"P407\", \"language of work\"),\n",
    "    Property(\"P452\", \"industry\"),\n",
    "    Property(\"P1128\", \"employees\"),\n",
    "    Property(\"P123\", \"publisher\"),\n",
    "    Property(\"P131\", \"is located in\"),\n",
    "    Property(\"P101\", \"field of work\"),\n",
    "    Property(\"P641\", \"sport\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712c6e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def extract_props_from_claims(claims, props):\n",
    "    extracted = {prop.label: [] for prop in props}\n",
    "        \n",
    "    for prop in props:\n",
    "        prop_value = prop.find_in_claims(claims)\n",
    "        for val in prop_value:\n",
    "            if val not in extracted[prop.label]:\n",
    "                extracted[prop.label].append(val)\n",
    "    \n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5728467a-7bc3-4ae8-8dbc-5f0bf41addd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "with open(JOURNAL_PROPS, 'w') as fjournals, pd.read_json(JOURNAL_WIKIDATA, lines=True, chunksize=500) as wikidump:        \n",
    "    for chunk in tqdm(wikidump, total=int(41115/500)):\n",
    "        for _, row in chunk.iterrows():\n",
    "            props = extract_props_from_claims(row.claims, properties)\n",
    "            props[\"journal\"] = row.journal\n",
    "            props[\"journal_id\"] = row.journal_ids\n",
    "            props[\"journal_label\"] = row.journal_label\n",
    "            props[\"description\"] = row.description\n",
    "            \n",
    "            fjournals.write(json.dumps(props, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b92ba509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_columns(group, str_cols=['description', 'journal_label', 'journal_id'], index_col='journal'):    \n",
    "    \n",
    "    list_columns = [col for col in group.columns if col not in str_cols and col != index_col]\n",
    "    journal = {column: set() for column in list_columns}\n",
    "        \n",
    "    for _, row in group.iterrows():\n",
    "        for column in list_columns:\n",
    "            if column not in str_cols:\n",
    "                res = row[column]\n",
    "                \n",
    "                if res:\n",
    "                    journal[column] = journal[column] | set(res)\n",
    "    \n",
    "    journal = {key: list(val) for key, val in journal.items()}\n",
    "    \n",
    "    for column in str_cols:\n",
    "        cat = group[column].str.cat(sep='|')\n",
    "        journal[column] = cat.split('|')\n",
    "        \n",
    "    journal[index_col] = group[index_col].iloc[0]    \n",
    "    \n",
    "    return journal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9c4b8ef-4806-4a7e-b0ec-ede9f4994e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance of</th>\n",
       "      <th>part of</th>\n",
       "      <th>genre</th>\n",
       "      <th>original broadcaster</th>\n",
       "      <th>country</th>\n",
       "      <th>country of origin</th>\n",
       "      <th>owned by</th>\n",
       "      <th>headquaters location</th>\n",
       "      <th>inception_year</th>\n",
       "      <th>newspaper format</th>\n",
       "      <th>...</th>\n",
       "      <th>industry</th>\n",
       "      <th>employees</th>\n",
       "      <th>publisher</th>\n",
       "      <th>is located in</th>\n",
       "      <th>field of work</th>\n",
       "      <th>sport</th>\n",
       "      <th>journal</th>\n",
       "      <th>journal_id</th>\n",
       "      <th>journal_label</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Q1002697]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1974]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q1411739]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>people.com</td>\n",
       "      <td>Q33659</td>\n",
       "      <td>People</td>\n",
       "      <td>weekly American magazine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Q41298]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1977]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q519143]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>usmagazine.com</td>\n",
       "      <td>Q549578</td>\n",
       "      <td>Us Weekly</td>\n",
       "      <td>American magazine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Q5398426]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q186068]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>Q6317205</td>\n",
       "      <td>Justice with Judge Jeanine</td>\n",
       "      <td>television series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Q15416]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q186068]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>Q7304120</td>\n",
       "      <td>Red Eye w/Greg Gutfeld</td>\n",
       "      <td>US television program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Q5398426]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q186068]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>Q17027753</td>\n",
       "      <td>The Real Story</td>\n",
       "      <td>television series</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  instance of part of genre original broadcaster country country of origin  \\\n",
       "0  [Q1002697]      []    []                   []   [Q30]             [Q30]   \n",
       "1    [Q41298]      []    []                   []      []             [Q30]   \n",
       "2  [Q5398426]      []    []            [Q186068]      []             [Q30]   \n",
       "3    [Q15416]      []    []            [Q186068]      []             [Q30]   \n",
       "4  [Q5398426]      []    []            [Q186068]      []             [Q30]   \n",
       "\n",
       "  owned by headquaters location inception_year newspaper format  ... industry  \\\n",
       "0       []                   []         [1974]               []  ...       []   \n",
       "1       []                   []         [1977]               []  ...       []   \n",
       "2       []                   []             []               []  ...       []   \n",
       "3       []                   []             []               []  ...       []   \n",
       "4       []                   []             []               []  ...       []   \n",
       "\n",
       "  employees   publisher is located in field of work sport         journal  \\\n",
       "0        []  [Q1411739]            []            []    []      people.com   \n",
       "1        []   [Q519143]            []            []    []  usmagazine.com   \n",
       "2        []          []            []            []    []     foxnews.com   \n",
       "3        []          []            []            []    []     foxnews.com   \n",
       "4        []          []            []            []    []     foxnews.com   \n",
       "\n",
       "  journal_id               journal_label               description  \n",
       "0     Q33659                      People  weekly American magazine  \n",
       "1    Q549578                   Us Weekly         American magazine  \n",
       "2   Q6317205  Justice with Judge Jeanine         television series  \n",
       "3   Q7304120      Red Eye w/Greg Gutfeld     US television program  \n",
       "4  Q17027753              The Real Story         television series  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journal_attrs = pd.read_json(JOURNAL_PROPS, lines=True)\n",
    "journal_attrs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e187430d-adac-42c0-8a22-1ed5ea01961f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance of</th>\n",
       "      <th>part of</th>\n",
       "      <th>genre</th>\n",
       "      <th>original broadcaster</th>\n",
       "      <th>country</th>\n",
       "      <th>country of origin</th>\n",
       "      <th>owned by</th>\n",
       "      <th>headquaters location</th>\n",
       "      <th>inception_year</th>\n",
       "      <th>newspaper format</th>\n",
       "      <th>language of work</th>\n",
       "      <th>industry</th>\n",
       "      <th>employees</th>\n",
       "      <th>publisher</th>\n",
       "      <th>is located in</th>\n",
       "      <th>field of work</th>\n",
       "      <th>sport</th>\n",
       "      <th>description</th>\n",
       "      <th>journal_label</th>\n",
       "      <th>journal_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journal</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>securitymiddleeast.com</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newstribune.com</th>\n",
       "      <td>[Q1002697]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q7948868]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q665319]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Jefferson City News Tribune]</td>\n",
       "      <td>[Q20710733]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thetelegram.com</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usgamer.net</th>\n",
       "      <td>[Q72398691]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[2013]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q1860]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[defunct video game news website]</td>\n",
       "      <td>[USgamer]</td>\n",
       "      <td>[Q73939073]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gizbot.com</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outsidethebeltway.com</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vestaviavoice.com</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox21online.com</th>\n",
       "      <td>[Q1616075]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1994]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Fox television affiliate in Duluth, Minnesota...</td>\n",
       "      <td>[KQDS-TV]</td>\n",
       "      <td>[Q6336159]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boereport.com</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naroomanewsonline.com.au</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          instance of part of genre original broadcaster  \\\n",
       "journal                                                                    \n",
       "securitymiddleeast.com             []      []    []                   []   \n",
       "newstribune.com            [Q1002697]      []    []                   []   \n",
       "thetelegram.com                    []      []    []                   []   \n",
       "usgamer.net               [Q72398691]      []    []                   []   \n",
       "gizbot.com                         []      []    []                   []   \n",
       "outsidethebeltway.com              []      []    []                   []   \n",
       "vestaviavoice.com                  []      []    []                   []   \n",
       "fox21online.com            [Q1616075]      []    []                   []   \n",
       "boereport.com                      []      []    []                   []   \n",
       "naroomanewsonline.com.au           []      []    []                   []   \n",
       "\n",
       "                         country country of origin    owned by  \\\n",
       "journal                                                          \n",
       "securitymiddleeast.com        []                []          []   \n",
       "newstribune.com               []                []  [Q7948868]   \n",
       "thetelegram.com               []                []          []   \n",
       "usgamer.net                   []                []          []   \n",
       "gizbot.com                    []                []          []   \n",
       "outsidethebeltway.com         []                []          []   \n",
       "vestaviavoice.com             []                []          []   \n",
       "fox21online.com            [Q30]                []          []   \n",
       "boereport.com                 []                []          []   \n",
       "naroomanewsonline.com.au      []                []          []   \n",
       "\n",
       "                         headquaters location inception_year newspaper format  \\\n",
       "journal                                                                         \n",
       "securitymiddleeast.com                     []             []               []   \n",
       "newstribune.com                            []             []        [Q665319]   \n",
       "thetelegram.com                            []             []               []   \n",
       "usgamer.net                                []         [2013]               []   \n",
       "gizbot.com                                 []             []               []   \n",
       "outsidethebeltway.com                      []             []               []   \n",
       "vestaviavoice.com                          []             []               []   \n",
       "fox21online.com                            []         [1994]               []   \n",
       "boereport.com                              []             []               []   \n",
       "naroomanewsonline.com.au                   []             []               []   \n",
       "\n",
       "                         language of work industry employees publisher  \\\n",
       "journal                                                                  \n",
       "securitymiddleeast.com                 []       []        []        []   \n",
       "newstribune.com                        []       []        []        []   \n",
       "thetelegram.com                        []       []        []        []   \n",
       "usgamer.net                       [Q1860]       []        []        []   \n",
       "gizbot.com                             []       []        []        []   \n",
       "outsidethebeltway.com                  []       []        []        []   \n",
       "vestaviavoice.com                      []       []        []        []   \n",
       "fox21online.com                        []       []        []        []   \n",
       "boereport.com                          []       []        []        []   \n",
       "naroomanewsonline.com.au               []       []        []        []   \n",
       "\n",
       "                         is located in field of work sport  \\\n",
       "journal                                                      \n",
       "securitymiddleeast.com              []            []    []   \n",
       "newstribune.com                     []            []    []   \n",
       "thetelegram.com                     []            []    []   \n",
       "usgamer.net                         []            []    []   \n",
       "gizbot.com                          []            []    []   \n",
       "outsidethebeltway.com               []            []    []   \n",
       "vestaviavoice.com                   []            []    []   \n",
       "fox21online.com                     []            []    []   \n",
       "boereport.com                       []            []    []   \n",
       "naroomanewsonline.com.au            []            []    []   \n",
       "\n",
       "                                                                description  \\\n",
       "journal                                                                       \n",
       "securitymiddleeast.com                                                   []   \n",
       "newstribune.com                                                          []   \n",
       "thetelegram.com                                                          []   \n",
       "usgamer.net                               [defunct video game news website]   \n",
       "gizbot.com                                                               []   \n",
       "outsidethebeltway.com                                                    []   \n",
       "vestaviavoice.com                                                        []   \n",
       "fox21online.com           [Fox television affiliate in Duluth, Minnesota...   \n",
       "boereport.com                                                            []   \n",
       "naroomanewsonline.com.au                                                 []   \n",
       "\n",
       "                                          journal_label   journal_id  \n",
       "journal                                                               \n",
       "securitymiddleeast.com                               []           []  \n",
       "newstribune.com           [Jefferson City News Tribune]  [Q20710733]  \n",
       "thetelegram.com                                      []           []  \n",
       "usgamer.net                                   [USgamer]  [Q73939073]  \n",
       "gizbot.com                                           []           []  \n",
       "outsidethebeltway.com                                []           []  \n",
       "vestaviavoice.com                                    []           []  \n",
       "fox21online.com                               [KQDS-TV]   [Q6336159]  \n",
       "boereport.com                                        []           []  \n",
       "naroomanewsonline.com.au                             []           []  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = journal_attrs.groupby('journal').apply(combine_columns)\n",
    "grouped = pd.DataFrame(grouped.to_list()).set_index('journal')\n",
    "\n",
    "grouped.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a28582c-8593-434d-86e7-948420957510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped.to_json(JOURNAL_ATTRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15bd932b-20b6-40e3-b920-40a6f261d843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance of</th>\n",
       "      <th>part of</th>\n",
       "      <th>genre</th>\n",
       "      <th>original broadcaster</th>\n",
       "      <th>country</th>\n",
       "      <th>country of origin</th>\n",
       "      <th>owned by</th>\n",
       "      <th>headquaters location</th>\n",
       "      <th>inception_year</th>\n",
       "      <th>newspaper format</th>\n",
       "      <th>language of work</th>\n",
       "      <th>industry</th>\n",
       "      <th>employees</th>\n",
       "      <th>publisher</th>\n",
       "      <th>is located in</th>\n",
       "      <th>field of work</th>\n",
       "      <th>sport</th>\n",
       "      <th>description</th>\n",
       "      <th>journal_label</th>\n",
       "      <th>journal_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1011now.com</th>\n",
       "      <td>[Q1616075]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q28260]</td>\n",
       "      <td>[1953]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[CBS television affiliate in Lincoln, Nebraska...</td>\n",
       "      <td>[KOLN]</td>\n",
       "      <td>[Q6334998]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070thefan.com</th>\n",
       "      <td>[Q14350]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q5373589]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[2007]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q1415]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[radio station from Indianapolis]</td>\n",
       "      <td>[WFNI]</td>\n",
       "      <td>[Q7949418]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107jamz.com</th>\n",
       "      <td>[Q14350]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q7830269]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q1588]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Urban contemporary radio station in Lake Arth...</td>\n",
       "      <td>[KJMH]</td>\n",
       "      <td>[Q6331512]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10news.com</th>\n",
       "      <td>[Q1616075]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q283457]</td>\n",
       "      <td>[Q16552]</td>\n",
       "      <td>[1953, 1997]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[channel 10 television station in San Diego, C...</td>\n",
       "      <td>[KGTV, KZSD-LP]</td>\n",
       "      <td>[Q3191396, Q16931724]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130thetiger.com</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zalebs.com</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zawya.com</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zdnet.com</th>\n",
       "      <td>[Q35127, Q4830453]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q7305191]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1991]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[business technology news website]</td>\n",
       "      <td>[ZDNet]</td>\n",
       "      <td>[Q2457578]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zeibiz.com</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zerotackle.com</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5734 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         instance of part of genre original broadcaster  \\\n",
       "1011now.com               [Q1616075]      []    []                   []   \n",
       "1070thefan.com              [Q14350]      []    []                   []   \n",
       "107jamz.com                 [Q14350]      []    []                   []   \n",
       "10news.com                [Q1616075]      []    []                   []   \n",
       "1130thetiger.com                  []      []    []                   []   \n",
       "...                              ...     ...   ...                  ...   \n",
       "zalebs.com                        []      []    []                   []   \n",
       "zawya.com                         []      []    []                   []   \n",
       "zdnet.com         [Q35127, Q4830453]      []    []                   []   \n",
       "zeibiz.com                        []      []    []                   []   \n",
       "zerotackle.com                    []      []    []                   []   \n",
       "\n",
       "                 country country of origin    owned by headquaters location  \\\n",
       "1011now.com        [Q30]                []          []             [Q28260]   \n",
       "1070thefan.com     [Q30]                []  [Q5373589]                   []   \n",
       "107jamz.com        [Q30]                []  [Q7830269]                   []   \n",
       "10news.com         [Q30]                []   [Q283457]             [Q16552]   \n",
       "1130thetiger.com      []                []          []                   []   \n",
       "...                  ...               ...         ...                  ...   \n",
       "zalebs.com            []                []          []                   []   \n",
       "zawya.com             []                []          []                   []   \n",
       "zdnet.com          [Q30]                []  [Q7305191]                   []   \n",
       "zeibiz.com            []                []          []                   []   \n",
       "zerotackle.com        []                []          []                   []   \n",
       "\n",
       "                 inception_year newspaper format language of work industry  \\\n",
       "1011now.com              [1953]               []               []       []   \n",
       "1070thefan.com           [2007]               []               []       []   \n",
       "107jamz.com                  []               []               []       []   \n",
       "10news.com         [1953, 1997]               []               []       []   \n",
       "1130thetiger.com             []               []               []       []   \n",
       "...                         ...              ...              ...      ...   \n",
       "zalebs.com                   []               []               []       []   \n",
       "zawya.com                    []               []               []       []   \n",
       "zdnet.com                [1991]               []               []       []   \n",
       "zeibiz.com                   []               []               []       []   \n",
       "zerotackle.com               []               []               []       []   \n",
       "\n",
       "                 employees publisher is located in field of work sport  \\\n",
       "1011now.com             []        []            []            []    []   \n",
       "1070thefan.com          []        []       [Q1415]            []    []   \n",
       "107jamz.com             []        []       [Q1588]            []    []   \n",
       "10news.com              []        []            []            []    []   \n",
       "1130thetiger.com        []        []            []            []    []   \n",
       "...                    ...       ...           ...           ...   ...   \n",
       "zalebs.com              []        []            []            []    []   \n",
       "zawya.com               []        []            []            []    []   \n",
       "zdnet.com               []        []            []            []    []   \n",
       "zeibiz.com              []        []            []            []    []   \n",
       "zerotackle.com          []        []            []            []    []   \n",
       "\n",
       "                                                        description  \\\n",
       "1011now.com       [CBS television affiliate in Lincoln, Nebraska...   \n",
       "1070thefan.com                    [radio station from Indianapolis]   \n",
       "107jamz.com       [Urban contemporary radio station in Lake Arth...   \n",
       "10news.com        [channel 10 television station in San Diego, C...   \n",
       "1130thetiger.com                                                 []   \n",
       "...                                                             ...   \n",
       "zalebs.com                                                       []   \n",
       "zawya.com                                                        []   \n",
       "zdnet.com                        [business technology news website]   \n",
       "zeibiz.com                                                       []   \n",
       "zerotackle.com                                                   []   \n",
       "\n",
       "                    journal_label             journal_id  \n",
       "1011now.com                [KOLN]             [Q6334998]  \n",
       "1070thefan.com             [WFNI]             [Q7949418]  \n",
       "107jamz.com                [KJMH]             [Q6331512]  \n",
       "10news.com        [KGTV, KZSD-LP]  [Q3191396, Q16931724]  \n",
       "1130thetiger.com               []                     []  \n",
       "...                           ...                    ...  \n",
       "zalebs.com                     []                     []  \n",
       "zawya.com                      []                     []  \n",
       "zdnet.com                 [ZDNet]             [Q2457578]  \n",
       "zeibiz.com                     []                     []  \n",
       "zerotackle.com                 []                     []  \n",
       "\n",
       "[5734 rows x 20 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json(JOURNAL_ATTRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b91b7b4-653d-4ffb-a444-09757da3accf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wikidata-handling.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/epfl-ada/ada-2021-project-improvise-adapt-overcome/blob/master/src/wikidata-handling.ipynb",
     "timestamp": 1636574601478
    }
   ]
  },
  "interpreter": {
   "hash": "dc2d54b6d87f225c775854c61ea7473bfcbf4d7f337d5b1c9b54ba2dedde31a4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
