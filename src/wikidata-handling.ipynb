{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77ac3c36",
   "metadata": {
    "id": "Rh9HNGSyY-kr"
   },
   "source": [
    "# ADA Project - Wikidata news outlet dataset generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287c36f-fe50-448a-a823-74897a9dbc6b",
   "metadata": {},
   "source": [
    "The goal of this notebook is to do to things:\n",
    "1. Identify the Wikidata QID for as many news outlets as possible, based on their URL in Quotebank.\n",
    "2. Extract useful features about those journals from Wikidata to generate a journal attribute dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfc5f24",
   "metadata": {
    "id": "OmVyXMtcowtX"
   },
   "source": [
    "## Setup and Remote dataset loading\n",
    "\n",
    "We first import necessary libraries into the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a7b8a8",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1636737257809,
     "user": {
      "displayName": "Luca Bataillard",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09240833841167909174"
     },
     "user_tz": -60
    },
    "id": "VjWVrwYpc1TR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pywikibot as pw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e5a5d9-f642-430c-898c-18f18d5ca49a",
   "metadata": {},
   "source": [
    "If necessary (i.e. running in Google Colab), we install the correct version of the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796e518c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7872,
     "status": "ok",
     "timestamp": 1636737266585,
     "user": {
      "displayName": "Luca Bataillard",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09240833841167909174"
     },
     "user_tz": -60
    },
    "id": "IeLtqc7vAXVp",
    "outputId": "ce2c7c15-7134-4e0f-e3ca-70b7374b9413"
   },
   "outputs": [],
   "source": [
    "%pip install pandas==1.3.0\n",
    "%pip install tld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffe8993",
   "metadata": {
    "id": "jEZb44cqcz68"
   },
   "source": [
    "We mount the EPFL google drive and define access paths for the different datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e1143f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2657,
     "status": "ok",
     "timestamp": 1636737328496,
     "user": {
      "displayName": "Luca Bataillard",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09240833841167909174"
     },
     "user_tz": -60
    },
    "id": "b6825e50",
    "outputId": "4c6aaa5f-6794-4e38-a058-ee953e0d1f51"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "447b7085",
   "metadata": {
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1636737352790,
     "user": {
      "displayName": "Luca Bataillard",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09240833841167909174"
     },
     "user_tz": -60
    },
    "id": "oO-QPsZ6Zbec"
   },
   "outputs": [],
   "source": [
    "# Data stored on disk\n",
    "BASE_PATH = \"../data/mnt/ada/\" \n",
    "# Data stored in Google Drive\n",
    "# BASE_PATH = \"/content/drive/Shareddrives/Improvise ADApt Overcome/Datasets/\"\n",
    "\n",
    "SPEAKER_PATH = BASE_PATH + \"speakers/\"\n",
    "QUOTEBANK_PATH = BASE_PATH + \"quotebank/\"\n",
    "WIKI_PATH = BASE_PATH + \"wikipedia/\"\n",
    "NEWS_PATH = BASE_PATH + \"newspapers/\"\n",
    "\n",
    "SPEAKER_ATTRS = SPEAKER_PATH + \"speaker_attributes.parquet\"\n",
    "WIKIPEDIA_ATTRS = SPEAKER_PATH + \"wikidata_labels_descriptions.csv.bz2\"\n",
    "QB_WIKIPEDIA_ATTRS = SPEAKER_PATH + \"wikidata_labels_descriptions_quotebank.csv.bz2\"\n",
    "FULL_WIKIDUMP = WIKI_PATH + \"wikidata-20211004-all.json.gz\"\n",
    "WIKI_URLS = WIKI_PATH + \"wikiurls.json\"\n",
    "\n",
    "CLEAN_WIKI_URLS = WIKI_PATH + \"clean_urls.json\"\n",
    "CLEAN_QUOTES = QUOTEBANK_PATH + \"clean_quotes.csv.bz2\"\n",
    "WIKI_QUOTES = NEWS_PATH + \"clean_wiki_quotes.csv.bz2\"\n",
    "JOURNAL_WIKIDATA = WIKI_PATH + \"journal_wikidump.json\"\n",
    "JOURNAL_PROPS = WIKI_PATH + \"journal_props.json\"\n",
    "JOURNAL_ATTRS = NEWS_PATH + \"journal_attributes.json\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032139b6",
   "metadata": {
    "id": "HYktWM7tFjN8"
   },
   "source": [
    "To classify speakers, we will use the `speaker_attributes` parquet file as is without further modification. Its size is small enough to be managable in RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69f90e5",
   "metadata": {
    "id": "VCc_QrfrF0Rn"
   },
   "source": [
    "## Newspaper URL extraction\n",
    "\n",
    "We need to generate a dataset linking news agency urls with their respective wikidata entry ids. This will allow us in the future to find patterns in the groups of news outlets by having more data about them. Once the wikidata id is obtained, it will relatively easy to obtain more information about those media outlets.\n",
    "\n",
    "Such a table is much reduced in size compared to a full wikipedia dump, since entries are restricted to news organizations. This means the dataset can be easily used in RAM, indexed by url in pandas, as a lookup table for publisher identifiers. This is a parallelizable task, so quotebank can be split into managable chunks to add a link between quote id and publisher for each quote. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e0341",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 16132,
     "status": "error",
     "timestamp": 1636737375151,
     "user": {
      "displayName": "Luca Bataillard",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09240833841167909174"
     },
     "user_tz": -60
    },
    "id": "3V2opxwcdlUm",
    "outputId": "2a8d8956-16e2-4ead-9f7d-67d54b6168ec"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse\n",
    "from tld import get_fld\n",
    "\n",
    "website_prop = \"P856\"\n",
    "\n",
    "def has_website(s):\n",
    "    return len(s.get(\"claims\", {}).get(website_prop, [])) > 0\n",
    "\n",
    "def extract_urls(s):\n",
    "    \"\"\"Takes a JSON wikidata entry and returns the list of official websites \n",
    "    linkedto that entry. The websises are returned as unique first-level-domains: \n",
    "    'https://test.google.com/exam/pl/e' becomes 'google.com'. Returns an empty \n",
    "    list if no urls\"\"\"\n",
    "\n",
    "    # Contains an official website?\n",
    "    if len(s.get(\"claims\", {}).get(website_prop, [])) > 0:\n",
    "        urls = []\n",
    "        flds = []\n",
    "\n",
    "    # For each website, add fld to urls array\n",
    "    for v in s[\"claims\"][website_prop]:\n",
    "        if (v[\"mainsnak\"].get(\"datavalue\", {}).get(\"value\", None) is not None):\n",
    "            url = urlparse(v[\"mainsnak\"][\"datavalue\"][\"value\"]).netloc\n",
    "\n",
    "            fld = get_fld(v[\"mainsnak\"][\"datavalue\"][\"value\"], fail_silently=True)\n",
    "\n",
    "            if fld and fld not in flds:\n",
    "                flds.append(fld)\n",
    "\n",
    "            if url not in urls:\n",
    "                urls.append(url)\n",
    "\n",
    "            return urls, flds\n",
    "        else:\n",
    "            return [], []\n",
    "\n",
    "\n",
    "def extract_newspaper_urls(inputf, outputf):\n",
    "    \"\"\"Takes an input wikidata dump file and writes a list of newspaper website\n",
    "    URLs. It filters entries based on if they are media companies. \n",
    "    The output file is a list of json objects, where each line is a json object. \n",
    "    Each object in the output file references a news agency, with \n",
    "    - \"id\" wikidata identifier of the news agency\n",
    "    - \"label\" wikidata label of the news agency\n",
    "    - \"websites\" list of urls related to that news agency\"\"\"\n",
    "    # Do not enforce encoding here since the input encoding is correct\n",
    "    with open(outputf, \"w\") as output_file:\n",
    "        with gzip.open(inputf, 'rb') as s_file:\n",
    "            for instance in s_file:\n",
    "                instance = instance.decode('utf-8')\n",
    "                instance = instance[:-2]\n",
    "                if len(instance)==0:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    s = json.loads(instance.strip(\"\\n\"))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                if s.get(\"labels\", {}).get(\"en\") is not None:\n",
    "                    s[\"label\"] = s[\"labels\"][\"en\"][\"value\"]\n",
    "\n",
    "                if s.get(\"labels\") is not None:\n",
    "                    del s[\"labels\"]\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                # Only take wiki entries with a website into consideration\n",
    "                if not has_website(s):\n",
    "                    continue\n",
    "\n",
    "                # Extract Official website \n",
    "                s[\"websites\"], s[\"flds\"] = extract_urls(s)\n",
    "\n",
    "                # Remove leftovers and unnecessary attributes\n",
    "                if s.get(\"aliases\") is not None:\n",
    "                    del s[\"aliases\"]\n",
    "                if s.get(\"descriptions\") is not None:\n",
    "                    del s[\"descriptions\"]\n",
    "                if s.get(\"sitelinks\") is not None:\n",
    "                    del s[\"sitelinks\"]\n",
    "                if s.get(\"claims\") is not None:\n",
    "                    del s[\"claims\"]\n",
    "                if s.get(\"lastrevid\") is not None:\n",
    "                    del s[\"lastrevid\"]\n",
    "                if s.get(\"type\") is not None:\n",
    "                    del s[\"type\"]\n",
    "\n",
    "                output_file.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "extract_newspaper_urls(FULL_WIKIDUMP, WIKI_URLS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152559e8",
   "metadata": {},
   "source": [
    "Go through every entry in the Wiki URL data to flatten `{id: ..., label: ..., flds: [...]}` into a list of `{id: ..., label: ..., flds: ...}, ...`. This is done after the fact due to the extremely long running time of the previous operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023df9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WIKI_URLS) as url_file, open(CLEAN_WIKI_URLS, 'w') as clean_urls:\n",
    "    for line in url_file:\n",
    "        urlDict = json.loads(line)\n",
    "        \n",
    "        for url in urlDict[\"flds\"]:\n",
    "            new_url = {\n",
    "                \"id\": urlDict[\"id\"],\n",
    "                \"url\": url\n",
    "            }\n",
    "            \n",
    "            if \"label\" in urlDict:\n",
    "                new_url[\"label\"] = urlDict[\"label\"]\n",
    "            \n",
    "            clean_urls.write(json.dumps(new_url, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcb4da7",
   "metadata": {},
   "source": [
    "We load the flattened URL mapping and index by URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9233bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q31</td>\n",
       "      <td>belgium.be</td>\n",
       "      <td>Belgium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q42</td>\n",
       "      <td>douglasadams.com</td>\n",
       "      <td>Douglas Adams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q2013</td>\n",
       "      <td>wikidata.org</td>\n",
       "      <td>Wikidata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q45</td>\n",
       "      <td>portugal.gov.pt</td>\n",
       "      <td>Portugal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q148</td>\n",
       "      <td>www.gov.cn</td>\n",
       "      <td>People's Republic of China</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id               url                       label\n",
       "0    Q31        belgium.be                     Belgium\n",
       "1    Q42  douglasadams.com               Douglas Adams\n",
       "2  Q2013      wikidata.org                    Wikidata\n",
       "3    Q45   portugal.gov.pt                    Portugal\n",
       "4   Q148        www.gov.cn  People's Republic of China"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_urls = pd.read_json(CLEAN_WIKI_URLS, lines=True)\n",
    "wiki_urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16a6eb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>belgium.be</th>\n",
       "      <td>Q31</td>\n",
       "      <td>Belgium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>douglasadams.com</th>\n",
       "      <td>Q42</td>\n",
       "      <td>Douglas Adams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikidata.org</th>\n",
       "      <td>Q2013</td>\n",
       "      <td>Wikidata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>portugal.gov.pt</th>\n",
       "      <td>Q45</td>\n",
       "      <td>Portugal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>www.gov.cn</th>\n",
       "      <td>Q148</td>\n",
       "      <td>People's Republic of China</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                       label\n",
       "url                                                \n",
       "belgium.be          Q31                     Belgium\n",
       "douglasadams.com    Q42               Douglas Adams\n",
       "wikidata.org      Q2013                    Wikidata\n",
       "portugal.gov.pt     Q45                    Portugal\n",
       "www.gov.cn         Q148  People's Republic of China"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_urls.set_index('url', inplace=True)\n",
    "wiki_urls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a935a943",
   "metadata": {},
   "source": [
    "## Merging quotes and URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be7d1e",
   "metadata": {},
   "source": [
    "We now need to add a wikidata id columns to the quotes dataset. This is done by filtering the wiki urls\n",
    "to keep only the URLS that appear in the quote dataset. We then left outer join the quotes and the wiki urls on\n",
    "the URL column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7e097ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-24-000168</td>\n",
       "      <td>Q20684375</td>\n",
       "      <td>2020-01-24 20:37:09</td>\n",
       "      <td>people.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-24-000168</td>\n",
       "      <td>Q20684375</td>\n",
       "      <td>2020-01-24 20:37:09</td>\n",
       "      <td>people.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-24-000168</td>\n",
       "      <td>Q20684375</td>\n",
       "      <td>2020-01-24 20:37:09</td>\n",
       "      <td>people.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-21-031706</td>\n",
       "      <td>Q20684375</td>\n",
       "      <td>2020-01-21 22:56:34</td>\n",
       "      <td>people.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-24-004947</td>\n",
       "      <td>Q20684375</td>\n",
       "      <td>2020-01-24 20:37:09</td>\n",
       "      <td>people.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID       qids                 date     journal\n",
       "0  2020-01-24-000168  Q20684375  2020-01-24 20:37:09  people.com\n",
       "1  2020-01-24-000168  Q20684375  2020-01-24 20:37:09  people.com\n",
       "2  2020-01-24-000168  Q20684375  2020-01-24 20:37:09  people.com\n",
       "3  2020-01-21-031706  Q20684375  2020-01-21 22:56:34  people.com\n",
       "4  2020-01-24-004947  Q20684375  2020-01-24 20:37:09  people.com"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes = pd.read_csv(CLEAN_QUOTES)\n",
    "quotes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df325b7c",
   "metadata": {},
   "source": [
    "Keep only the wikiurl entries appearing in the quotes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c6b8066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people.com</td>\n",
       "      <td>Q33659</td>\n",
       "      <td>People</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>usmagazine.com</td>\n",
       "      <td>Q549578</td>\n",
       "      <td>Us Weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>Q6317205</td>\n",
       "      <td>Justice with Judge Jeanine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>Q7304120</td>\n",
       "      <td>Red Eye w/Greg Gutfeld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>Q17027753</td>\n",
       "      <td>The Real Story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              url         id                       label\n",
       "0      people.com     Q33659                      People\n",
       "1  usmagazine.com    Q549578                   Us Weekly\n",
       "2     foxnews.com   Q6317205  Justice with Judge Jeanine\n",
       "2     foxnews.com   Q7304120      Red Eye w/Greg Gutfeld\n",
       "2     foxnews.com  Q17027753              The Real Story"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journal_urls = pd.DataFrame(quotes.journal.unique(), columns=['url'])\n",
    "\n",
    "journals = journal_urls.merge(wiki_urls, left_on='url', right_index=True, how='left')\n",
    "journals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73afc731-fbd0-48ba-beab-117904a27c3d",
   "metadata": {},
   "source": [
    "As an example, we have that `foxnews.com` appears many times in the dataset. This is due to our URL shortening technique, which will make `hannity.foxnews.com` map to `foxnews.com`. Since _Hannity_ has its own Wikidata entry, it will appear as a separate row in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd84f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>Q7488834</td>\n",
       "      <td>Shannon Bream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>Q54958468</td>\n",
       "      <td>Life, Liberty &amp; Levin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>Q2842798</td>\n",
       "      <td>America's Newsroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>Q7628156</td>\n",
       "      <td>America Reports With John Roberts &amp; Sandra Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>Q8058978</td>\n",
       "      <td>Your World with Neil Cavuto</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           url         id                                             label\n",
       "2  foxnews.com   Q7488834                                     Shannon Bream\n",
       "2  foxnews.com  Q54958468                             Life, Liberty & Levin\n",
       "2  foxnews.com   Q2842798                                America's Newsroom\n",
       "2  foxnews.com   Q7628156  America Reports With John Roberts & Sandra Smith\n",
       "2  foxnews.com   Q8058978                       Your World with Neil Cavuto"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journals.query('url == \"foxnews.com\"').sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f980ee6-a637-4d93-bdbb-4ac3955e0de9",
   "metadata": {},
   "source": [
    "We clean up the column names and reindex our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4d991d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal_id</th>\n",
       "      <th>journal_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journal</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>people.com</th>\n",
       "      <td>Q33659</td>\n",
       "      <td>People</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usmagazine.com</th>\n",
       "      <td>Q549578</td>\n",
       "      <td>Us Weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foxnews.com</th>\n",
       "      <td>Q6317205</td>\n",
       "      <td>Justice with Judge Jeanine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foxnews.com</th>\n",
       "      <td>Q7304120</td>\n",
       "      <td>Red Eye w/Greg Gutfeld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foxnews.com</th>\n",
       "      <td>Q17027753</td>\n",
       "      <td>The Real Story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               journal_id               journal_label\n",
       "journal                                              \n",
       "people.com         Q33659                      People\n",
       "usmagazine.com    Q549578                   Us Weekly\n",
       "foxnews.com      Q6317205  Justice with Judge Jeanine\n",
       "foxnews.com      Q7304120      Red Eye w/Greg Gutfeld\n",
       "foxnews.com     Q17027753              The Real Story"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journals.rename(columns={'url': 'journal', 'id': 'journal_id', 'label': 'journal_label'}, inplace=True)\n",
    "journals.set_index('journal', inplace=True)\n",
    "journals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9890fd08",
   "metadata": {},
   "source": [
    "Now that we have a URL -> QID mapping, we will query wikipedia to extract hopefully meaningful features into the dataset. We do notice that the same URL can direct to several wikipedia entries. We will take care of this in a later part of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56ccab5",
   "metadata": {},
   "source": [
    "## Enhance quote dataset with source journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb101fc-73c9-4e21-93f7-2554ea23bbe3",
   "metadata": {},
   "source": [
    "We merge each quote with its journal entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07bee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_quotes = quotes.merge(journals, left_on='journal', how='left', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c16300e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>quoteID</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>journal</th>\n",
       "      <th>journal_id</th>\n",
       "      <th>journal_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3860581</th>\n",
       "      <td>3860581</td>\n",
       "      <td>2020-02-09-019782</td>\n",
       "      <td>Q4726265</td>\n",
       "      <td>2020-02-09 18:44:08</td>\n",
       "      <td>stroudnewsandjournal.co.uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7682397</th>\n",
       "      <td>7682397</td>\n",
       "      <td>2020-01-30-095775</td>\n",
       "      <td>Q419976</td>\n",
       "      <td>2020-01-30 22:49:00</td>\n",
       "      <td>eveningtimes.co.uk</td>\n",
       "      <td>Q5416645</td>\n",
       "      <td>Glasgow Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6691802</th>\n",
       "      <td>6691802</td>\n",
       "      <td>2020-02-16-026839</td>\n",
       "      <td>Q359442</td>\n",
       "      <td>2020-02-16 00:00:00</td>\n",
       "      <td>sandiegouniontribune.com</td>\n",
       "      <td>Q3547109</td>\n",
       "      <td>San Diego Union-Tribune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7391225</th>\n",
       "      <td>7391225</td>\n",
       "      <td>2020-03-23-021545</td>\n",
       "      <td>Q20682472</td>\n",
       "      <td>2020-03-23 10:41:46</td>\n",
       "      <td>biv.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303827</th>\n",
       "      <td>2303827</td>\n",
       "      <td>2020-02-18-043485</td>\n",
       "      <td>Q311440</td>\n",
       "      <td>2020-02-18 00:00:00</td>\n",
       "      <td>sandiegouniontribune.com</td>\n",
       "      <td>Q3547109</td>\n",
       "      <td>San Diego Union-Tribune</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0            quoteID       qids                 date  \\\n",
       "3860581     3860581  2020-02-09-019782   Q4726265  2020-02-09 18:44:08   \n",
       "7682397     7682397  2020-01-30-095775    Q419976  2020-01-30 22:49:00   \n",
       "6691802     6691802  2020-02-16-026839    Q359442  2020-02-16 00:00:00   \n",
       "7391225     7391225  2020-03-23-021545  Q20682472  2020-03-23 10:41:46   \n",
       "2303827     2303827  2020-02-18-043485    Q311440  2020-02-18 00:00:00   \n",
       "\n",
       "                            journal journal_id            journal_label  \n",
       "3860581  stroudnewsandjournal.co.uk        NaN                      NaN  \n",
       "7682397          eveningtimes.co.uk   Q5416645            Glasgow Times  \n",
       "6691802    sandiegouniontribune.com   Q3547109  San Diego Union-Tribune  \n",
       "7391225                     biv.com        NaN                      NaN  \n",
       "2303827    sandiegouniontribune.com   Q3547109  San Diego Union-Tribune  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_quotes.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7535680b",
   "metadata": {},
   "source": [
    "We notice that about 72% of all quotes have an associated Wikidata QID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ffdf0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.08% of all quotes have a wikipedia editor identified\n"
     ]
    }
   ],
   "source": [
    "pct_wiki = 1 - wiki_quotes.journal_id.isna().sum() / len(wiki_quotes.journal_id)\n",
    "print(f\"{pct_wiki * 100:.2f}% of all quotes have a wikipedia editor identified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c3b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_quotes.to_csv(WIKI_QUOTES, compression='bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0483f917",
   "metadata": {},
   "source": [
    "## Generate journal enhanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabfe1a5",
   "metadata": {},
   "source": [
    "Now that we have extracted QIDs for the news outlets in our dataset, we will try and extract useful features for each of them.\n",
    "\n",
    "We start off by saving the full Wikidata page for each journal QID to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc4e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_helpers import download_wiki_entry, get_item_claims_from_wiki\n",
    "\n",
    "def write_wikidump(file, journal, desc, claims):\n",
    "    \"\"\"Takes a wikidata description and claims object, a URL and writes it to a file\"\"\"\n",
    "    output = {}\n",
    "    \n",
    "    output[\"journal\"] = journal.journal\n",
    "    output[\"journal_ids\"] = journal.journal_id\n",
    "    output[\"journal_label\"] = journal.journal_label\n",
    "    output[\"description\"] = desc\n",
    "    output[\"claims\"] = claims\n",
    "    \n",
    "    file.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(JOURNAL_WIKIDATA, 'w') as fjournals:        \n",
    "    for _, journal in tqdm(journals.reset_index().iterrows(), total=len(journals)):\n",
    "        desc, claims = download_wiki_entry(journal.journal_id)\n",
    "        write_wikidump(fjournals, journal, desc, claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f0f2f1",
   "metadata": {},
   "source": [
    "We define a list of properties that seems relevant for journals and their respective custom extractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32baf50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_extractor(raw_prop_val):\n",
    "    \"\"\"Takes a wikidata prop object and extracts the year from a potential time field\"\"\"\n",
    "    if not (raw_prop_val and 'time' in raw_prop_val):\n",
    "        return np.nan\n",
    "    \n",
    "    try:\n",
    "        raw_year = raw_prop_val['time'].split('-')[0]\n",
    "        return int(raw_year.lstrip('+').lstrip('0'))\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4d4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_helpers import Property\n",
    "\n",
    "# Define useful properties we want to enhance our dataset with\n",
    "properties = [\n",
    "    Property(\"P31\",\"instance of\"),\n",
    "    Property(\"P361\", \"part of\"),\n",
    "    Property(\"P136\", \"genre\"),\n",
    "    Property(\"P449\", \"original broadcaster\"),\n",
    "    Property(\"P17\", \"country\"),\n",
    "    Property(\"P495\", \"country of origin\"),\n",
    "    Property(\"P127\", \"owned by\"),\n",
    "    Property(\"P159\", \"headquaters location\"),\n",
    "    Property(\"P571\", \"inception_year\", extractor=year_extractor),\n",
    "    Property(\"P3912\", \"newspaper format\"),\n",
    "    Property(\"P407\", \"language of work\"),\n",
    "    Property(\"P452\", \"industry\"),\n",
    "    Property(\"P1128\", \"employees\"),\n",
    "    Property(\"P123\", \"publisher\"),\n",
    "    Property(\"P131\", \"is located in\"),\n",
    "    Property(\"P101\", \"field of work\"),\n",
    "    Property(\"P641\", \"sport\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566cbd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_props_from_claims(claims, props):\n",
    "    \"\"\"Takes a list of wikidata claims and a list of properties and extracts those properties\n",
    "    from \"\"\"\n",
    "    extracted = {prop.label: [] for prop in props}\n",
    "        \n",
    "    for prop in props:\n",
    "        prop_value = prop.find_in_claims(claims)\n",
    "        for val in prop_value:\n",
    "            if val not in extracted[prop.label]:\n",
    "                extracted[prop.label].append(val)\n",
    "    \n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f1e66",
   "metadata": {},
   "source": [
    "We run through the full journal Wikidump, extract the properties and write them to a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e610a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(JOURNAL_PROPS, 'w') as fjournals, pd.read_json(JOURNAL_WIKIDATA, lines=True, chunksize=500) as wikidump:        \n",
    "    for chunk in tqdm(wikidump, total=int(41115/500)):\n",
    "        for _, row in chunk.iterrows():\n",
    "            props = extract_props_from_claims(row.claims, properties)\n",
    "            props[\"journal\"] = row.journal\n",
    "            props[\"journal_id\"] = row.journal_ids\n",
    "            props[\"journal_label\"] = row.journal_label\n",
    "            props[\"description\"] = row.description\n",
    "            \n",
    "            fjournals.write(json.dumps(props, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327dd2c",
   "metadata": {},
   "source": [
    "The only remaining thing to do is to make the index unique by combining all the entries with the same journal URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0778327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_columns(group, str_cols=['description', 'journal_label', 'journal_id'], index_col='journal'):    \n",
    "    \"\"\"Takes a group of rows and combines their columns so that they form a single row. \n",
    "    String columns are concatenated with '|' and regular columns are combined into a list\"\"\"\n",
    "    \n",
    "    list_columns = [col for col in group.columns if col not in str_cols and col != index_col]\n",
    "    journal = {column: set() for column in list_columns}\n",
    "        \n",
    "    for _, row in group.iterrows():\n",
    "        for column in list_columns:\n",
    "            if column not in str_cols:\n",
    "                res = row[column]\n",
    "                \n",
    "                if res:\n",
    "                    journal[column] = journal[column] | set(res)\n",
    "    \n",
    "    journal = {key: list(val) for key, val in journal.items()}\n",
    "    \n",
    "    for column in str_cols:\n",
    "        cat = group[column].str.cat(sep='|')\n",
    "        journal[column] = cat.split('|')\n",
    "        \n",
    "    journal[index_col] = group[index_col].iloc[0]    \n",
    "    \n",
    "    return journal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d68df08",
   "metadata": {},
   "source": [
    "Before combining, we notice that `foxnews.com`has three entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d16ddd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance of</th>\n",
       "      <th>part of</th>\n",
       "      <th>genre</th>\n",
       "      <th>original broadcaster</th>\n",
       "      <th>country</th>\n",
       "      <th>country of origin</th>\n",
       "      <th>owned by</th>\n",
       "      <th>headquaters location</th>\n",
       "      <th>inception_year</th>\n",
       "      <th>newspaper format</th>\n",
       "      <th>...</th>\n",
       "      <th>industry</th>\n",
       "      <th>employees</th>\n",
       "      <th>publisher</th>\n",
       "      <th>is located in</th>\n",
       "      <th>field of work</th>\n",
       "      <th>sport</th>\n",
       "      <th>journal</th>\n",
       "      <th>journal_id</th>\n",
       "      <th>journal_label</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Q1002697]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1974]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q1411739]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>people.com</td>\n",
       "      <td>Q33659</td>\n",
       "      <td>People</td>\n",
       "      <td>weekly American magazine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Q41298]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1977]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q519143]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>usmagazine.com</td>\n",
       "      <td>Q549578</td>\n",
       "      <td>Us Weekly</td>\n",
       "      <td>American magazine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Q5398426]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q186068]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>Q6317205</td>\n",
       "      <td>Justice with Judge Jeanine</td>\n",
       "      <td>television series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Q15416]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q186068]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>Q7304120</td>\n",
       "      <td>Red Eye w/Greg Gutfeld</td>\n",
       "      <td>US television program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Q5398426]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q186068]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>Q17027753</td>\n",
       "      <td>The Real Story</td>\n",
       "      <td>television series</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  instance of part of genre original broadcaster country country of origin  \\\n",
       "0  [Q1002697]      []    []                   []   [Q30]             [Q30]   \n",
       "1    [Q41298]      []    []                   []      []             [Q30]   \n",
       "2  [Q5398426]      []    []            [Q186068]      []             [Q30]   \n",
       "3    [Q15416]      []    []            [Q186068]      []             [Q30]   \n",
       "4  [Q5398426]      []    []            [Q186068]      []             [Q30]   \n",
       "\n",
       "  owned by headquaters location inception_year newspaper format  ... industry  \\\n",
       "0       []                   []         [1974]               []  ...       []   \n",
       "1       []                   []         [1977]               []  ...       []   \n",
       "2       []                   []             []               []  ...       []   \n",
       "3       []                   []             []               []  ...       []   \n",
       "4       []                   []             []               []  ...       []   \n",
       "\n",
       "  employees   publisher is located in field of work sport         journal  \\\n",
       "0        []  [Q1411739]            []            []    []      people.com   \n",
       "1        []   [Q519143]            []            []    []  usmagazine.com   \n",
       "2        []          []            []            []    []     foxnews.com   \n",
       "3        []          []            []            []    []     foxnews.com   \n",
       "4        []          []            []            []    []     foxnews.com   \n",
       "\n",
       "  journal_id               journal_label               description  \n",
       "0     Q33659                      People  weekly American magazine  \n",
       "1    Q549578                   Us Weekly         American magazine  \n",
       "2   Q6317205  Justice with Judge Jeanine         television series  \n",
       "3   Q7304120      Red Eye w/Greg Gutfeld     US television program  \n",
       "4  Q17027753              The Real Story         television series  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journal_attrs = pd.read_json(JOURNAL_PROPS, lines=True)\n",
    "journal_attrs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b983bd3",
   "metadata": {},
   "source": [
    "We obtain our final feature dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7a536ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance of</th>\n",
       "      <th>part of</th>\n",
       "      <th>genre</th>\n",
       "      <th>original broadcaster</th>\n",
       "      <th>country</th>\n",
       "      <th>country of origin</th>\n",
       "      <th>owned by</th>\n",
       "      <th>headquaters location</th>\n",
       "      <th>inception_year</th>\n",
       "      <th>newspaper format</th>\n",
       "      <th>language of work</th>\n",
       "      <th>industry</th>\n",
       "      <th>employees</th>\n",
       "      <th>publisher</th>\n",
       "      <th>is located in</th>\n",
       "      <th>field of work</th>\n",
       "      <th>sport</th>\n",
       "      <th>description</th>\n",
       "      <th>journal_label</th>\n",
       "      <th>journal_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journal</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>securitymiddleeast.com</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newstribune.com</th>\n",
       "      <td>[Q1002697]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q7948868]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q665319]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Jefferson City News Tribune]</td>\n",
       "      <td>[Q20710733]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thetelegram.com</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usgamer.net</th>\n",
       "      <td>[Q72398691]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[2013]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q1860]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[defunct video game news website]</td>\n",
       "      <td>[USgamer]</td>\n",
       "      <td>[Q73939073]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gizbot.com</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outsidethebeltway.com</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vestaviavoice.com</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox21online.com</th>\n",
       "      <td>[Q1616075]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1994]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Fox television affiliate in Duluth, Minnesota...</td>\n",
       "      <td>[KQDS-TV]</td>\n",
       "      <td>[Q6336159]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boereport.com</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naroomanewsonline.com.au</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          instance of part of genre original broadcaster  \\\n",
       "journal                                                                    \n",
       "securitymiddleeast.com             []      []    []                   []   \n",
       "newstribune.com            [Q1002697]      []    []                   []   \n",
       "thetelegram.com                    []      []    []                   []   \n",
       "usgamer.net               [Q72398691]      []    []                   []   \n",
       "gizbot.com                         []      []    []                   []   \n",
       "outsidethebeltway.com              []      []    []                   []   \n",
       "vestaviavoice.com                  []      []    []                   []   \n",
       "fox21online.com            [Q1616075]      []    []                   []   \n",
       "boereport.com                      []      []    []                   []   \n",
       "naroomanewsonline.com.au           []      []    []                   []   \n",
       "\n",
       "                         country country of origin    owned by  \\\n",
       "journal                                                          \n",
       "securitymiddleeast.com        []                []          []   \n",
       "newstribune.com               []                []  [Q7948868]   \n",
       "thetelegram.com               []                []          []   \n",
       "usgamer.net                   []                []          []   \n",
       "gizbot.com                    []                []          []   \n",
       "outsidethebeltway.com         []                []          []   \n",
       "vestaviavoice.com             []                []          []   \n",
       "fox21online.com            [Q30]                []          []   \n",
       "boereport.com                 []                []          []   \n",
       "naroomanewsonline.com.au      []                []          []   \n",
       "\n",
       "                         headquaters location inception_year newspaper format  \\\n",
       "journal                                                                         \n",
       "securitymiddleeast.com                     []             []               []   \n",
       "newstribune.com                            []             []        [Q665319]   \n",
       "thetelegram.com                            []             []               []   \n",
       "usgamer.net                                []         [2013]               []   \n",
       "gizbot.com                                 []             []               []   \n",
       "outsidethebeltway.com                      []             []               []   \n",
       "vestaviavoice.com                          []             []               []   \n",
       "fox21online.com                            []         [1994]               []   \n",
       "boereport.com                              []             []               []   \n",
       "naroomanewsonline.com.au                   []             []               []   \n",
       "\n",
       "                         language of work industry employees publisher  \\\n",
       "journal                                                                  \n",
       "securitymiddleeast.com                 []       []        []        []   \n",
       "newstribune.com                        []       []        []        []   \n",
       "thetelegram.com                        []       []        []        []   \n",
       "usgamer.net                       [Q1860]       []        []        []   \n",
       "gizbot.com                             []       []        []        []   \n",
       "outsidethebeltway.com                  []       []        []        []   \n",
       "vestaviavoice.com                      []       []        []        []   \n",
       "fox21online.com                        []       []        []        []   \n",
       "boereport.com                          []       []        []        []   \n",
       "naroomanewsonline.com.au               []       []        []        []   \n",
       "\n",
       "                         is located in field of work sport  \\\n",
       "journal                                                      \n",
       "securitymiddleeast.com              []            []    []   \n",
       "newstribune.com                     []            []    []   \n",
       "thetelegram.com                     []            []    []   \n",
       "usgamer.net                         []            []    []   \n",
       "gizbot.com                          []            []    []   \n",
       "outsidethebeltway.com               []            []    []   \n",
       "vestaviavoice.com                   []            []    []   \n",
       "fox21online.com                     []            []    []   \n",
       "boereport.com                       []            []    []   \n",
       "naroomanewsonline.com.au            []            []    []   \n",
       "\n",
       "                                                                description  \\\n",
       "journal                                                                       \n",
       "securitymiddleeast.com                                                   []   \n",
       "newstribune.com                                                          []   \n",
       "thetelegram.com                                                          []   \n",
       "usgamer.net                               [defunct video game news website]   \n",
       "gizbot.com                                                               []   \n",
       "outsidethebeltway.com                                                    []   \n",
       "vestaviavoice.com                                                        []   \n",
       "fox21online.com           [Fox television affiliate in Duluth, Minnesota...   \n",
       "boereport.com                                                            []   \n",
       "naroomanewsonline.com.au                                                 []   \n",
       "\n",
       "                                          journal_label   journal_id  \n",
       "journal                                                               \n",
       "securitymiddleeast.com                               []           []  \n",
       "newstribune.com           [Jefferson City News Tribune]  [Q20710733]  \n",
       "thetelegram.com                                      []           []  \n",
       "usgamer.net                                   [USgamer]  [Q73939073]  \n",
       "gizbot.com                                           []           []  \n",
       "outsidethebeltway.com                                []           []  \n",
       "vestaviavoice.com                                    []           []  \n",
       "fox21online.com                               [KQDS-TV]   [Q6336159]  \n",
       "boereport.com                                        []           []  \n",
       "naroomanewsonline.com.au                             []           []  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = journal_attrs.groupby('journal').apply(combine_columns)\n",
    "grouped = pd.DataFrame(grouped.to_list()).set_index('journal')\n",
    "\n",
    "grouped.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f07796b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped.to_json(JOURNAL_ATTRS)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wikidata-handling.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/epfl-ada/ada-2021-project-improvise-adapt-overcome/blob/master/src/wikidata-handling.ipynb",
     "timestamp": 1636574601478
    }
   ]
  },
  "interpreter": {
   "hash": "dc2d54b6d87f225c775854c61ea7473bfcbf4d7f337d5b1c9b54ba2dedde31a4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
