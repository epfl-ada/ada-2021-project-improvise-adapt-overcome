{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rh9HNGSyY-kr"
   },
   "source": [
    "# ADA Project Milestone 2 - Wikidata dataset newspaper generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmVyXMtcowtX"
   },
   "source": [
    "## Setup and Remote dataset loading\n",
    "\n",
    "We first import necessary libraries into the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1636737257809,
     "user": {
      "displayName": "Luca Bataillard",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09240833841167909174"
     },
     "user_tz": -60
    },
    "id": "VjWVrwYpc1TR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7872,
     "status": "ok",
     "timestamp": 1636737266585,
     "user": {
      "displayName": "Luca Bataillard",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09240833841167909174"
     },
     "user_tz": -60
    },
    "id": "IeLtqc7vAXVp",
    "outputId": "ce2c7c15-7134-4e0f-e3ca-70b7374b9413"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.3.0 in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.0) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.0) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.0) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.0) (1.15.0)\n",
      "Requirement already satisfied: tld in /usr/local/lib/python3.7/dist-packages (0.12.6)\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas==1.3.0\n",
    "%pip install tld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEZb44cqcz68"
   },
   "source": [
    "We mount the EPFL google drive and define access paths for the different datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2657,
     "status": "ok",
     "timestamp": 1636737328496,
     "user": {
      "displayName": "Luca Bataillard",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09240833841167909174"
     },
     "user_tz": -60
    },
    "id": "b6825e50",
    "outputId": "4c6aaa5f-6794-4e38-a058-ee953e0d1f51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1636737352790,
     "user": {
      "displayName": "Luca Bataillard",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09240833841167909174"
     },
     "user_tz": -60
    },
    "id": "oO-QPsZ6Zbec"
   },
   "outputs": [],
   "source": [
    "#BASE_PATH = \"../data/mnt/ada/\"\n",
    "BASE_PATH = \"/content/drive/Shareddrives/Improvise ADApt Overcome/Datasets/\"\n",
    "\n",
    "SPEAKER_PATH = BASE_PATH + \"speakers/\"\n",
    "QUOTEBANK_PATH = BASE_PATH + \"quotebank/\"\n",
    "WIKI_PATH = BASE_PATH + \"wikipedia/\"\n",
    "NEWS_PATH = BASE_PATH + \"newspapers/\"\n",
    "\n",
    "SPEAKER_ATTRS = SPEAKER_PATH + \"speaker_attributes.parquet\"\n",
    "WIKIPEDIA_ATTRS = SPEAKER_PATH + \"wikidata_labels_descriptions.csv.bz2\"\n",
    "QB_WIKIPEDIA_ATTRS = SPEAKER_PATH + \"wikidata_labels_descriptions_quotebank.csv.bz2\"\n",
    "FULL_WIKIDUMP = WIKI_PATH + \"wikipedia-latest-all.json.bz2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYktWM7tFjN8"
   },
   "source": [
    "To classify speakers, we will use the `speaker_attributes` parquet file as is without further modification. Its size is small enough to be managable in RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCc_QrfrF0Rn"
   },
   "source": [
    "## Newspaper URL extraction\n",
    "\n",
    "We need to generate a dataset linking news agency urls with their respective wikidata entry ids. This will allow us in the future to find patterns in the groups of news outlets by having more data about them. Once the wikidata id is obtained, it will relatively easy to obtain more information about those media outlets.\n",
    "\n",
    "Such a table is much reduced in size compared to a full wikipedia dump, since entries are restricted to news organizations. This means the dataset can be easily used in RAM, indexed by url in pandas, as a lookup table for publisher identifiers. This is a parallelizable task, so quotebank can be split into managable chunks to add a link between quote id and publisher for each quote. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 16132,
     "status": "error",
     "timestamp": 1636737375151,
     "user": {
      "displayName": "Luca Bataillard",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09240833841167909174"
     },
     "user_tz": -60
    },
    "id": "3V2opxwcdlUm",
    "outputId": "2a8d8956-16e2-4ead-9f7d-67d54b6168ec"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "from tld import get_fld\n",
    "\n",
    "def is_media_company(s):\n",
    "  \"\"\"Takes a JSON wikidata entry and returns True if that entry is an\n",
    "  instance of media company, False otherwise\"\"\" \n",
    "  instanceof_prop = \"P31\"\n",
    "  media_company = \"Q1331793\"\n",
    "\n",
    "  if len(s.get(\"claims\", {}).get(instanceof_prop, [])) > 0:\n",
    "    # Check all \"instance-of\" properties, return true if s is instance of media company\n",
    "    instancesof = []\n",
    "    for v in s[\"claims\"][instanceof_prop]:\n",
    "        # id: \"Q123\", \"numeric-id\": 123\n",
    "        if (\n",
    "            v[\"mainsnak\"].get(\"datavalue\", {}).get(\"value\", {}).get(\"id\")\n",
    "            is not None\n",
    "        ):\n",
    "            instancesof.append(v[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"])\n",
    "    \n",
    "    return media_company in instancesof\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "\n",
    "def extract_urls(s):\n",
    "  \"\"\"Takes a JSON wikidata entry and returns the list of official websites \n",
    "  linkedto that entry. The websises are returned as unique first-level-domains: \n",
    "  'https://test.google.com/exam/pl/e' becomes 'google.com'. Returns an empty \n",
    "  list if  \"\"\"\n",
    "  website_prop = \"P856\"\n",
    "\n",
    "  # Contains an official website?\n",
    "  if len(s.get(\"claims\", {}).get(website_prop, [])) > 0:\n",
    "    urls = []\n",
    "\n",
    "    # For each website, add fld to urls array\n",
    "    for v in s[\"claims\"][website_prop]:\n",
    "      if (v[\"mainsnak\"].get(\"datavalue\", {}).get(\"value\", {}) is not None):\n",
    "        url = get_fld(v[\"mainsnak\"][\"datavalue\"][\"value\"])\n",
    "        if url not in urls:\n",
    "          urls.append(url)\n",
    "\n",
    "    return list(urls)\n",
    "  else:\n",
    "    return []\n",
    "\n",
    "\n",
    "def extract_newspaper_urls(inputf, outputf):\n",
    "  \"\"\"Takes an input wikidata dump file and writes a list of newspaper website\n",
    "  URLs. It filters entries based on if they are media companies. \n",
    "  The output file is a list of json objects, where each line is a json object. \n",
    "  Each object in the output file references a news agency, with \n",
    "    - \"id\" wikidata identifier of the news agency\n",
    "    - \"label\" wikidata label of the news agency\n",
    "    - \"websites\" list of urls related to that news agency\"\"\"\n",
    "  # Do not enforce encoding here since the input encoding is correct\n",
    "  with open(outputf, \"w\") as output_file:\n",
    "      with bz2.open(inputf, 'rb') as s_file:\n",
    "          for instance in s_file:\n",
    "              instance = instance.decode('utf-8')\n",
    "              instance = instance[:-2]\n",
    "              if len(instance)==0:\n",
    "                  continue\n",
    "              s = json.loads(instance.strip(\"\\n\"))\n",
    "\n",
    "              if s.get(\"labels\", {}).get(\"en\") is not None:\n",
    "                  s[\"label\"] = s[\"labels\"][\"en\"][\"value\"]\n",
    "              \n",
    "              if s.get(\"labels\") is not None:\n",
    "                  del s[\"labels\"]\n",
    "              else:\n",
    "                  continue\n",
    "\n",
    "              # Only take media companies into consideration\n",
    "              if not is_media_company(s):\n",
    "                continue\n",
    "                \n",
    "              # Extract Official website \n",
    "              s[\"websites\"] = extract_urls(s)\n",
    "\n",
    "              # Remove leftovers and unnecessary attributes\n",
    "              if s.get(\"aliases\") is not None:\n",
    "                  del s[\"aliases\"]\n",
    "              if s.get(\"descriptions\") is not None:\n",
    "                  del s[\"descriptions\"]\n",
    "              if s.get(\"sitelinks\") is not None:\n",
    "                  del s[\"sitelinks\"]\n",
    "              if s.get(\"claims\") is not None:\n",
    "                  del s[\"claims\"]\n",
    "              if s.get(\"lastrevid\") is not None:\n",
    "                  del s[\"lastrevid\"]\n",
    "              if s.get(\"type\") is not None:\n",
    "                  del s[\"type\"]\n",
    "\n",
    "              output_file.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "extract_newspaper_urls(FULL_WIKIDUMP, NEWS_URLS)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wikidata-handling.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/epfl-ada/ada-2021-project-improvise-adapt-overcome/blob/master/src/wikidata-handling.ipynb",
     "timestamp": 1636574601478
    }
   ]
  },
  "interpreter": {
   "hash": "dc2d54b6d87f225c775854c61ea7473bfcbf4d7f337d5b1c9b54ba2dedde31a4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
